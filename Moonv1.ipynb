{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZOnFzSVkm7GHolLvSPT3Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntMilton/MoonIa/blob/main/Moonv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wPi-uJz6I_jg",
        "outputId": "6f08ff45-71b6-406f-eac2-688c25050e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n",
            "\n",
            "--- A simular dados de exemplo... ---\n",
            "Dados simulados criados com 10000 registos.\n",
            "!! Lembre-se de substituir este bloco pelos seus dados reais. !!\n",
            "\n",
            "--- A iniciar a Engenharia de Atributos... ---\n",
            "Engenharia de Atributos concluída.\n",
            "\n",
            "--- A definir a complexidade das transações... ---\n",
            "Transações classificadas: 2149 complexas e 7851 simples.\n",
            "\n",
            "--- A pré-processar e a dividir os dados... ---\n",
            "Dados divididos em conjuntos de treino e teste para ambos os modelos.\n",
            "\n",
            "--- A treinar o modelo Isolation Forest... ---\n",
            "Modelo Isolation Forest treinado e predições realizadas.\n",
            "\n",
            "--- A construir e treinar o modelo Autoencoder... ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m112\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m144\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m102\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m494\u001b[0m (1.93 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">494</span> (1.93 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m494\u001b[0m (1.93 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">494</span> (1.93 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.1450 - val_loss: 0.0983\n",
            "Epoch 2/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0881 - val_loss: 0.0565\n",
            "Epoch 3/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0521 - val_loss: 0.0315\n",
            "Epoch 4/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0241 - val_loss: 0.0068\n",
            "Epoch 5/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0060 - val_loss: 0.0039\n",
            "Epoch 6/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0031\n",
            "Epoch 7/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0030 - val_loss: 0.0027\n",
            "Epoch 8/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0020 - val_loss: 0.0026\n",
            "Epoch 9/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 10/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0018 - val_loss: 0.0023\n",
            "Epoch 11/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0020 - val_loss: 0.0022\n",
            "Epoch 12/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 13/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 14/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 15/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 16/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0010 - val_loss: 0.0018\n",
            "Epoch 17/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 18/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 19/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.3162e-04 - val_loss: 0.0015\n",
            "Epoch 20/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 21/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.2613e-04 - val_loss: 0.0011\n",
            "Epoch 22/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.6352e-04 - val_loss: 9.1945e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2399e-04 - val_loss: 8.1593e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5069e-04 - val_loss: 7.1926e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6676e-04 - val_loss: 6.5527e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3884e-04 - val_loss: 6.0471e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5176e-04 - val_loss: 5.8138e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5070e-04 - val_loss: 5.3562e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6261e-04 - val_loss: 4.8093e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5368e-04 - val_loss: 4.5173e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4071e-04 - val_loss: 4.3677e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8650e-04 - val_loss: 4.0274e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9324e-04 - val_loss: 3.5450e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5588e-04 - val_loss: 3.2714e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2689e-04 - val_loss: 2.9806e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1852e-04 - val_loss: 2.8633e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3297e-04 - val_loss: 2.7419e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.9574e-05 - val_loss: 2.5785e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0224e-04 - val_loss: 2.4040e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.3566e-05 - val_loss: 2.3178e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3411e-04 - val_loss: 2.1737e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.6199e-05 - val_loss: 2.0686e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 8.6235e-05 - val_loss: 2.0253e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.2482e-05 - val_loss: 1.9117e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.4398e-05 - val_loss: 1.8041e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.3014e-05 - val_loss: 1.8167e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.7546e-05 - val_loss: 1.7166e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.8378e-05 - val_loss: 1.6121e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.3603e-05 - val_loss: 1.5609e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.0122e-05 - val_loss: 1.4746e-04\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Modelo Autoencoder treinado e predições realizadas.\n",
            "\n",
            "--- Resultados Finais ---\n",
            "\n",
            "O Isolation Forest detetou 28 anomalias em transações simples.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      id_transacao           data_hora     valor tipo_transacao     categoria  \\\n",
              "9381          9381 2024-10-13 23:00:00  71979.46  Transferência     Marketing   \n",
              "3672          3672 2024-02-04 23:00:00  73762.29  Transferência  Fornecedores   \n",
              "7437          7437 2024-03-21 18:00:00  81393.68  Transferência  Fornecedores   \n",
              "6680          6680 2024-10-13 02:00:00  76392.19    Recebimento  Fornecedores   \n",
              "6009          6009 2024-01-10 15:00:00  80697.25    Recebimento     Marketing   \n",
              "\n",
              "      id_entidade  desvio_media_historica  desvio_mediana_historica  \\\n",
              "9381           43                4.595297                 22.854993   \n",
              "3672           15                6.401311                 24.097580   \n",
              "7437           19                6.274835                 21.182648   \n",
              "6680           13                5.636293                 21.842300   \n",
              "6009           35                7.477505                 26.726726   \n",
              "\n",
              "      hora_incomum  dia_fim_de_semana  arredondamento_valor  sequencia_rapida  \\\n",
              "9381             1                  1                 False                 0   \n",
              "3672             1                  1                 False                 0   \n",
              "7437             0                  0                 False                 0   \n",
              "6680             1                  1                 False                 0   \n",
              "6009             0                  0                 False                 0   \n",
              "\n",
              "      complexidade  \n",
              "9381             0  \n",
              "3672             0  \n",
              "7437             0  \n",
              "6680             0  \n",
              "6009             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7bf7cf72-3626-4d0e-be81-5cb3d3538e8c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_transacao</th>\n",
              "      <th>data_hora</th>\n",
              "      <th>valor</th>\n",
              "      <th>tipo_transacao</th>\n",
              "      <th>categoria</th>\n",
              "      <th>id_entidade</th>\n",
              "      <th>desvio_media_historica</th>\n",
              "      <th>desvio_mediana_historica</th>\n",
              "      <th>hora_incomum</th>\n",
              "      <th>dia_fim_de_semana</th>\n",
              "      <th>arredondamento_valor</th>\n",
              "      <th>sequencia_rapida</th>\n",
              "      <th>complexidade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9381</th>\n",
              "      <td>9381</td>\n",
              "      <td>2024-10-13 23:00:00</td>\n",
              "      <td>71979.46</td>\n",
              "      <td>Transferência</td>\n",
              "      <td>Marketing</td>\n",
              "      <td>43</td>\n",
              "      <td>4.595297</td>\n",
              "      <td>22.854993</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3672</th>\n",
              "      <td>3672</td>\n",
              "      <td>2024-02-04 23:00:00</td>\n",
              "      <td>73762.29</td>\n",
              "      <td>Transferência</td>\n",
              "      <td>Fornecedores</td>\n",
              "      <td>15</td>\n",
              "      <td>6.401311</td>\n",
              "      <td>24.097580</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7437</th>\n",
              "      <td>7437</td>\n",
              "      <td>2024-03-21 18:00:00</td>\n",
              "      <td>81393.68</td>\n",
              "      <td>Transferência</td>\n",
              "      <td>Fornecedores</td>\n",
              "      <td>19</td>\n",
              "      <td>6.274835</td>\n",
              "      <td>21.182648</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6680</th>\n",
              "      <td>6680</td>\n",
              "      <td>2024-10-13 02:00:00</td>\n",
              "      <td>76392.19</td>\n",
              "      <td>Recebimento</td>\n",
              "      <td>Fornecedores</td>\n",
              "      <td>13</td>\n",
              "      <td>5.636293</td>\n",
              "      <td>21.842300</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6009</th>\n",
              "      <td>6009</td>\n",
              "      <td>2024-01-10 15:00:00</td>\n",
              "      <td>80697.25</td>\n",
              "      <td>Recebimento</td>\n",
              "      <td>Marketing</td>\n",
              "      <td>35</td>\n",
              "      <td>7.477505</td>\n",
              "      <td>26.726726</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bf7cf72-3626-4d0e-be81-5cb3d3538e8c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7bf7cf72-3626-4d0e-be81-5cb3d3538e8c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7bf7cf72-3626-4d0e-be81-5cb3d3538e8c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6f62eb46-13f6-4ef4-9955-1fac038fa101\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f62eb46-13f6-4ef4-9955-1fac038fa101')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6f62eb46-13f6-4ef4-9955-1fac038fa101 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\nAn\\u00e1lise conclu\\u00edda\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id_transacao\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2083,\n        \"min\": 3672,\n        \"max\": 9381,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3672,\n          6009,\n          7437\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data_hora\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-01-10 15:00:00\",\n        \"max\": \"2024-10-13 23:00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2024-02-04 23:00:00\",\n          \"2024-01-10 15:00:00\",\n          \"2024-03-21 18:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4150.660704433691,\n        \"min\": 71979.46,\n        \"max\": 81393.68,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          73762.29,\n          80697.25,\n          81393.68\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tipo_transacao\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Recebimento\",\n          \"Transfer\\u00eancia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categoria\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Fornecedores\",\n          \"Marketing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_entidade\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 13,\n        \"max\": 43,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          15,\n          35\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"desvio_media_historica\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.060117473335211,\n        \"min\": 4.595296996591609,\n        \"max\": 7.477504628732216,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          6.4013113440777785,\n          7.477504628732216\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"desvio_mediana_historica\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1895400758090373,\n        \"min\": 21.182647674270587,\n        \"max\": 26.726726098463566,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          24.097580137563682,\n          26.726726098463566\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hora_incomum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dia_fim_de_semana\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arredondamento_valor\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sequencia_rapida\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"complexidade\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "O Autoencoder detetou 33 anomalias em transações complexas.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      id_transacao           data_hora      valor tipo_transacao  \\\n",
              "4569          4569 2024-08-25 21:00:00  329861.72    Recebimento   \n",
              "9090          9090 2024-12-05 12:00:00  683680.54  Transferência   \n",
              "9575          9575 2024-10-13 01:00:00  142825.20  Transferência   \n",
              "6195          6195 2024-11-06 15:00:00  195090.44      Pagamento   \n",
              "216            216 2024-09-22 05:00:00  166143.77    Recebimento   \n",
              "\n",
              "         categoria  id_entidade  desvio_media_historica  \\\n",
              "4569  Fornecedores            2               34.492966   \n",
              "9090  Fornecedores           21               45.343893   \n",
              "9575      Salários           20               12.997951   \n",
              "6195  Fornecedores            7               23.196194   \n",
              "216   Fornecedores            7               19.606068   \n",
              "\n",
              "      desvio_mediana_historica  hora_incomum  dia_fim_de_semana  \\\n",
              "4569                127.449575             1                  1   \n",
              "9090                249.965619             0                  0   \n",
              "9575                 48.092887             1                  1   \n",
              "6195                 63.520862             0                  0   \n",
              "216                  53.947537             1                  1   \n",
              "\n",
              "      arredondamento_valor  sequencia_rapida  complexidade  \n",
              "4569                 False                 0             1  \n",
              "9090                 False                 0             1  \n",
              "9575                 False                 0             1  \n",
              "6195                 False                 0             1  \n",
              "216                  False                 0             1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-994245a5-ce33-493e-a769-83897a6e5510\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_transacao</th>\n",
              "      <th>data_hora</th>\n",
              "      <th>valor</th>\n",
              "      <th>tipo_transacao</th>\n",
              "      <th>categoria</th>\n",
              "      <th>id_entidade</th>\n",
              "      <th>desvio_media_historica</th>\n",
              "      <th>desvio_mediana_historica</th>\n",
              "      <th>hora_incomum</th>\n",
              "      <th>dia_fim_de_semana</th>\n",
              "      <th>arredondamento_valor</th>\n",
              "      <th>sequencia_rapida</th>\n",
              "      <th>complexidade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4569</th>\n",
              "      <td>4569</td>\n",
              "      <td>2024-08-25 21:00:00</td>\n",
              "      <td>329861.72</td>\n",
              "      <td>Recebimento</td>\n",
              "      <td>Fornecedores</td>\n",
              "      <td>2</td>\n",
              "      <td>34.492966</td>\n",
              "      <td>127.449575</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9090</th>\n",
              "      <td>9090</td>\n",
              "      <td>2024-12-05 12:00:00</td>\n",
              "      <td>683680.54</td>\n",
              "      <td>Transferência</td>\n",
              "      <td>Fornecedores</td>\n",
              "      <td>21</td>\n",
              "      <td>45.343893</td>\n",
              "      <td>249.965619</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9575</th>\n",
              "      <td>9575</td>\n",
              "      <td>2024-10-13 01:00:00</td>\n",
              "      <td>142825.20</td>\n",
              "      <td>Transferência</td>\n",
              "      <td>Salários</td>\n",
              "      <td>20</td>\n",
              "      <td>12.997951</td>\n",
              "      <td>48.092887</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6195</th>\n",
              "      <td>6195</td>\n",
              "      <td>2024-11-06 15:00:00</td>\n",
              "      <td>195090.44</td>\n",
              "      <td>Pagamento</td>\n",
              "      <td>Fornecedores</td>\n",
              "      <td>7</td>\n",
              "      <td>23.196194</td>\n",
              "      <td>63.520862</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>216</td>\n",
              "      <td>2024-09-22 05:00:00</td>\n",
              "      <td>166143.77</td>\n",
              "      <td>Recebimento</td>\n",
              "      <td>Fornecedores</td>\n",
              "      <td>7</td>\n",
              "      <td>19.606068</td>\n",
              "      <td>53.947537</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-994245a5-ce33-493e-a769-83897a6e5510')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-994245a5-ce33-493e-a769-83897a6e5510 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-994245a5-ce33-493e-a769-83897a6e5510');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2b888bb8-4978-4210-99f5-e0fdedd18147\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b888bb8-4978-4210-99f5-e0fdedd18147')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2b888bb8-4978-4210-99f5-e0fdedd18147 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\nAn\\u00e1lise conclu\\u00edda\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id_transacao\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3802,\n        \"min\": 216,\n        \"max\": 9575,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9090,\n          216,\n          9575\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data_hora\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-08-25 21:00:00\",\n        \"max\": \"2024-12-05 12:00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2024-12-05 12:00:00\",\n          \"2024-09-22 05:00:00\",\n          \"2024-10-13 01:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 224537.26816663353,\n        \"min\": 142825.2,\n        \"max\": 683680.54,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          683680.54,\n          166143.77,\n          142825.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tipo_transacao\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Recebimento\",\n          \"Transfer\\u00eancia\",\n          \"Pagamento\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categoria\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Sal\\u00e1rios\",\n          \"Fornecedores\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_entidade\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 2,\n        \"max\": 21,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          21,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"desvio_media_historica\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.823399766210237,\n        \"min\": 12.997950575300013,\n        \"max\": 45.34389264679668,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          45.34389264679668,\n          19.606067922442545\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"desvio_mediana_historica\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 85.17581188073484,\n        \"min\": 48.09288706740905,\n        \"max\": 249.9656191726138,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          249.9656191726138,\n          53.947537419982424\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hora_incomum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dia_fim_de_semana\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arredondamento_valor\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sequencia_rapida\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"complexidade\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Análise concluída. O próximo passo seria integrar estes resultados no dashboard.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS NECESSÁRIAS\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Bibliotecas de Machine Learning e Pré-processamento\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Bibliotecas para o Autoencoder (Deep Learning)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CARREGAMENTO DOS DADOS (SIMULAÇÃO)\n",
        "# !! SUBSTITUA ESTE BLOCO PELO SEU CÓDIGO DE CARREGAMENTO DE DADOS !!\n",
        "# ==============================================================================\n",
        "print(\"\\n--- A simular dados de exemplo... ---\")\n",
        "num_registos = 10000\n",
        "data = {\n",
        "    'id_transacao': range(num_registos),\n",
        "    'data_hora': pd.to_datetime(np.random.choice(pd.date_range('2024-01-01', '2025-01-01', freq='h'), num_registos)),\n",
        "    'valor': np.random.lognormal(mean=8, sigma=1.5, size=num_registos).round(2),\n",
        "    'tipo_transacao': np.random.choice(['Pagamento', 'Recebimento', 'Transferência'], size=num_registos),\n",
        "    'categoria': np.random.choice(['Fornecedores', 'Salários', 'Marketing', 'Diversos', 'Outros'], size=num_registos, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
        "    'id_entidade': np.random.randint(1, 50, size=num_registos)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Inserir alguns valores anómalos para teste\n",
        "df.loc[df.sample(frac=0.02).index, 'valor'] *= np.random.uniform(5, 10) # Valores muito altos\n",
        "df.loc[df.sample(frac=0.01).index, 'valor'] *= -1 # Valores negativos\n",
        "df.loc[df.sample(frac=0.01).index, 'data_hora'] += pd.to_timedelta(np.random.randint(21, 23), unit='h') # Horas incomuns\n",
        "print(f\"Dados simulados criados com {len(df)} registos.\")\n",
        "print(\"!! Lembre-se de substituir este bloco pelos seus dados reais. !!\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. ENGENHARIA DE ATRIBUTOS (FEATURE ENGINEERING)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- A iniciar a Engenharia de Atributos... ---\")\n",
        "\n",
        "# 3.1. Garantir que a coluna de data está no formato datetime\n",
        "df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "\n",
        "# 3.2. Refinamento de Features Existentes e Criação de Novas\n",
        "# Desvio em relação à média (como no seu TCC)\n",
        "media_por_entidade = df.groupby('id_entidade')['valor'].transform('mean')\n",
        "df['desvio_media_historica'] = (df['valor'] - media_por_entidade) / (media_por_entidade + 1e-6) # Adicionado 1e-6 para evitar divisão por zero\n",
        "\n",
        "# Desvio em relação à mediana (nova feature)\n",
        "mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "\n",
        "# Features contextuais baseadas em regras de auditoria\n",
        "df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "\n",
        "# Feature de sequência rápida (requer ordenação prévia)\n",
        "df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "\n",
        "# Limpeza de valores NaN que possam ter sido criados (ex: na primeira transação de uma entidade)\n",
        "df.fillna(0, inplace=True)\n",
        "print(\"Engenharia de Atributos concluída.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. DEFINIÇÃO DA COMPLEXIDADE DA TRANSAÇÃO\n",
        "# ==============================================================================\n",
        "print(\"\\n--- A definir a complexidade das transações... ---\")\n",
        "\n",
        "# Regra robusta com múltiplos critérios\n",
        "media_geral = df['valor'].mean()\n",
        "std_geral = df['valor'].std()\n",
        "\n",
        "# Critério 1: Valor excede 3 desvios padrão da média geral\n",
        "criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "\n",
        "# Critério 2: Categoria é de alto risco (\"Diversos\" ou \"Outros\")\n",
        "criterio_2 = df['categoria'].isin(['Diversos', 'Outros'])\n",
        "\n",
        "# Critério 3: O valor da transação é negativo\n",
        "criterio_3 = df['valor'] < 0\n",
        "\n",
        "# Aplicar a regra: se qualquer critério for verdadeiro, a transação é complexa\n",
        "df['complexidade'] = (criterio_1 | criterio_2 | criterio_3).astype(int) # 1 para complexa, 0 para simples\n",
        "\n",
        "num_complexas = df['complexidade'].sum()\n",
        "num_simples = len(df) - num_complexas\n",
        "print(f\"Transações classificadas: {num_complexas} complexas e {num_simples} simples.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. PRÉ-PROCESSAMENTO E DIVISÃO DOS DADOS\n",
        "# ==============================================================================\n",
        "print(\"\\n--- A pré-processar e a dividir os dados... ---\")\n",
        "\n",
        "# Selecionar as features numéricas para os modelos\n",
        "features_numericas = df.select_dtypes(include=np.number).columns.tolist()\n",
        "# Remover colunas que não são features, como IDs ou a própria label de complexidade\n",
        "features_a_remover = ['id_transacao', 'id_entidade', 'complexidade']\n",
        "features_para_modelo = [f for f in features_numericas if f not in features_a_remover]\n",
        "\n",
        "X = df[features_para_modelo]\n",
        "\n",
        "# Normalizar os dados\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "df_scaled['complexidade'] = df['complexidade'] # Adicionar a coluna de complexidade ao df escalado\n",
        "\n",
        "# Separar em dataframes para cada modelo\n",
        "df_simples = df_scaled[df_scaled['complexidade'] == 0].drop('complexidade', axis=1)\n",
        "df_complexas = df_scaled[df_scaled['complexidade'] == 1].drop('complexidade', axis=1)\n",
        "\n",
        "# Divisão Treino/Teste para ambos os conjuntos\n",
        "X_simples_train, X_simples_test = train_test_split(df_simples, test_size=0.3, random_state=42)\n",
        "X_complexas_train, X_complexas_test = train_test_split(df_complexas, test_size=0.3, random_state=42)\n",
        "print(\"Dados divididos em conjuntos de treino e teste para ambos os modelos.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. MODELO 1: ISOLATION FOREST (TRANSAÇÕES SIMPLES)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- A treinar o modelo Isolation Forest... ---\")\n",
        "\n",
        "# Instanciar o modelo com os hiperparâmetros otimizados\n",
        "iso_forest_otimizado = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Treinar o modelo\n",
        "iso_forest_otimizado.fit(X_simples_train)\n",
        "\n",
        "# Fazer predições (-1 para anomalia, 1 para normal)\n",
        "predicoes_if = iso_forest_otimizado.predict(X_simples_test)\n",
        "\n",
        "# Guardar os resultados\n",
        "resultados_if = X_simples_test.copy()\n",
        "resultados_if['anomalia_detectada'] = predicoes_if\n",
        "print(\"Modelo Isolation Forest treinado e predições realizadas.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. MODELO 2: AUTOENCODER (TRANSAÇÕES COMPLEXAS)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- A construir e treinar o modelo Autoencoder... ---\")\n",
        "\n",
        "# 7.1. Definir a nova arquitetura de rede neural profunda\n",
        "input_dim = X_complexas_train.shape[1]\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoder = Dense(16, activation='relu')(input_layer)\n",
        "encoder = Dense(8, activation='relu')(encoder) # Gargalo\n",
        "decoder = Dense(16, activation='relu')(encoder)\n",
        "decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "autoencoder_profundo = Model(inputs=input_layer, outputs=decoder)\n",
        "autoencoder_profundo.compile(optimizer='adam', loss='mean_squared_error')\n",
        "autoencoder_profundo.summary()\n",
        "\n",
        "# 7.2. Treinar o modelo\n",
        "autoencoder_profundo.fit(X_complexas_train, X_complexas_train,\n",
        "                         epochs=50,\n",
        "                         batch_size=32,\n",
        "                         shuffle=True,\n",
        "                         validation_data=(X_complexas_test, X_complexas_test),\n",
        "                         verbose=1)\n",
        "\n",
        "# 7.3. Fazer predições e calcular o erro de reconstrução\n",
        "reconstrucoes = autoencoder_profundo.predict(X_complexas_test)\n",
        "mse = np.mean(np.power(X_complexas_test - reconstrucoes, 2), axis=1)\n",
        "\n",
        "# 7.4. Aplicar o novo limiar de 95%\n",
        "threshold = np.quantile(mse, 0.95)\n",
        "anomalias_ae = (mse > threshold).astype(int) # 1 para anomalia, 0 para normal\n",
        "\n",
        "# Guardar os resultados\n",
        "resultados_ae = X_complexas_test.copy()\n",
        "resultados_ae['erro_reconstrucao'] = mse\n",
        "resultados_ae['anomalia_detectada'] = anomalias_ae.replace({0: 1, 1: -1}) # Alinhar com o formato do IF (-1 para anomalia)\n",
        "print(\"Modelo Autoencoder treinado e predições realizadas.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. ANÁLISE FINAL DOS RESULTADOS\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Resultados Finais ---\")\n",
        "\n",
        "# Anomalias detetadas pelo Isolation Forest\n",
        "anomalias_if_df = resultados_if[resultados_if['anomalia_detectada'] == -1]\n",
        "print(f\"\\nO Isolation Forest detetou {len(anomalias_if_df)} anomalias em transações simples.\")\n",
        "# Mostrar as transações originais (não escaladas)\n",
        "display(df.loc[anomalias_if_df.index].head())\n",
        "\n",
        "\n",
        "# Anomalias detetadas pelo Autoencoder\n",
        "anomalias_ae_df = resultados_ae[resultados_ae['anomalia_detectada'] == -1]\n",
        "print(f\"\\nO Autoencoder detetou {len(anomalias_ae_df)} anomalias em transações complexas.\")\n",
        "# Mostrar as transações originais (não escaladas)\n",
        "display(df.loc[anomalias_ae_df.index].head())\n",
        "\n",
        "print(\"\\nAnálise concluída. O próximo passo seria integrar estes resultados no dashboard.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "import dash\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Importações para o pipeline de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# (Esta função encapsula todo o processo que definimos anteriormente)\n",
        "# ==============================================================================\n",
        "\n",
        "def preparar_dados_para_dashboard():\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo: simulação de dados, engenharia de atributos,\n",
        "    modelagem e predição. Retorna um DataFrame final pronto para o dashboard.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A iniciar a preparação completa dos dados para o dashboard... ---\")\n",
        "\n",
        "    # --- Simulação de Dados (SUBSTITUA PELOS SEUS DADOS REAIS) ---\n",
        "    print(\"--- A simular dados de exemplo... ---\")\n",
        "    num_registos = 10000\n",
        "    data = {\n",
        "        'id_transacao': range(num_registos),\n",
        "        'data_hora': pd.to_datetime(np.random.choice(pd.date_range('2024-01-01', '2025-01-01', freq='h'), num_registos)),\n",
        "        'valor': np.random.lognormal(mean=8, sigma=1.5, size=num_registos).round(2),\n",
        "        'tipo_transacao': np.random.choice(['Pagamento', 'Recebimento', 'Transferência'], size=num_registos),\n",
        "        'categoria': np.random.choice(['Fornecedores', 'Salários', 'Marketing', 'Diversos', 'Outros'], size=num_registos, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
        "        'id_entidade': np.random.randint(1, 50, size=num_registos)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.loc[df.sample(frac=0.02).index, 'valor'] *= np.random.uniform(5, 10)\n",
        "    df.loc[df.sample(frac=0.01).index, 'valor'] *= -1\n",
        "    df.loc[df.sample(frac=0.01).index, 'data_hora'] += pd.to_timedelta(np.random.randint(21, 23), unit='h')\n",
        "    print(\"!! Lembre-se de substituir este bloco pelos seus dados reais. !!\")\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['categoria'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'])\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "\n",
        "    # Adicionar colunas originais necessárias para a lógica e o resultado final\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    # Isolation Forest\n",
        "    df_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "    iso_forest.fit(df_simples)\n",
        "    predicoes_if = iso_forest.predict(df_simples)\n",
        "\n",
        "    # Autoencoder\n",
        "    df_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "    else:\n",
        "        predicoes_ae = np.array([])\n",
        "\n",
        "\n",
        "    # --- Consolidação dos Resultados ---\n",
        "    print(\"--- Consolidando resultados... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    indices_simples = df_simples.index\n",
        "    df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "    df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    if not df_complexas.empty:\n",
        "        indices_complexas = df_complexas.index\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    print(\"Preparação dos dados concluída.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# Executar a função para obter o DataFrame final\n",
        "df_final = preparar_dados_para_dashboard()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "app = dash.Dash(__name__)\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = html.Div(style={'fontFamily': 'Arial, sans-serif', 'padding': '20px'}, children=[\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", style={'textAlign': 'center', 'color': '#003366'}),\n",
        "    html.H2(\"Caso de Estudo: System Business Development (SBD)\", style={'textAlign': 'center', 'color': '#505050'}),\n",
        "\n",
        "    html.Div(className='control-panel', style={'backgroundColor': '#f2f2f2', 'padding': '20px', 'borderRadius': '5px', 'marginBottom': '20px'}, children=[\n",
        "        html.H4(\"Filtros de Análise\", style={'marginTop': 0}),\n",
        "        html.Div(className='filters', style={'display': 'flex', 'gap': '40px'}, children=[\n",
        "            html.Div(children=[\n",
        "                html.Label(\"Estado da Anomalia:\"),\n",
        "                dcc.Dropdown(\n",
        "                    id='filtro-anomalia',\n",
        "                    options=[\n",
        "                        {'label': 'Todas', 'value': 'Todas'},\n",
        "                        {'label': 'Anómalas', 'value': 'Anómala'},\n",
        "                        {'label': 'Normais', 'value': 'Normal'}\n",
        "                    ],\n",
        "                    value='Todas'\n",
        "                )\n",
        "            ], style={'width': '33%'}),\n",
        "\n",
        "            html.Div(children=[\n",
        "                html.Label(\"Complexidade da Transação:\"),\n",
        "                dcc.Dropdown(\n",
        "                    id='filtro-complexidade',\n",
        "                    options=[\n",
        "                        {'label': 'Todas', 'value': 'Todas'},\n",
        "                        {'label': 'Simples', 'value': 'Simples'},\n",
        "                        {'label': 'Complexas', 'value': 'Complexa'}\n",
        "                    ],\n",
        "                    value='Todas'\n",
        "                )\n",
        "            ], style={'width': '33%'})\n",
        "        ])\n",
        "    ]),\n",
        "\n",
        "    # Gráfico de Dispersão\n",
        "    dcc.Graph(id='scatter-plot'),\n",
        "\n",
        "    # Tabela de Dados\n",
        "    html.Hr(),\n",
        "    html.H3(\"Detalhes das Transações Selecionadas\"),\n",
        "    dash_table.DataTable(\n",
        "        id='data-table',\n",
        "        columns=[{\"name\": i, \"id\": i} for i in df_final.columns],\n",
        "        page_size=10,\n",
        "        style_table={'overflowX': 'auto'},\n",
        "        style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "        style_cell={'textAlign': 'left', 'padding': '5px'},\n",
        "        sort_action=\"native\",\n",
        "        filter_action=\"native\",\n",
        "    )\n",
        "])\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. CALLBACKS - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "@app.callback(\n",
        "    Output('scatter-plot', 'figure'),\n",
        "    Output('data-table', 'data'),\n",
        "    Output('data-table', 'columns'),\n",
        "    Input('filtro-anomalia', 'value'),\n",
        "    Input('filtro-complexidade', 'value')\n",
        ")\n",
        "def update_dashboard(selected_anomalia, selected_complexidade):\n",
        "    \"\"\"\n",
        "    Atualiza o gráfico de dispersão e a tabela de dados com base nos filtros selecionados.\n",
        "    \"\"\"\n",
        "    filtered_df = df_final.copy()\n",
        "\n",
        "    # Aplicar filtro de anomalia\n",
        "    if selected_anomalia != 'Todas':\n",
        "        filtered_df = filtered_df[filtered_df['anomalia_detectada'] == selected_anomalia]\n",
        "\n",
        "    # Aplicar filtro de complexidade\n",
        "    if selected_complexidade != 'Todas':\n",
        "        filtered_df = filtered_df[filtered_df['complexidade'] == selected_complexidade]\n",
        "\n",
        "    # Criar gráfico de dispersão\n",
        "    fig = px.scatter(\n",
        "        filtered_df,\n",
        "        x=\"valor\",\n",
        "        y=\"desvio_media_historica\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['id_transacao', 'data_hora', 'tipo_transacao', 'categoria', 'id_entidade', 'complexidade', 'modelo_deteccao'],\n",
        "        title=\"Transações por Valor vs. Desvio da Média (Coloridas por Anomalia)\"\n",
        "    )\n",
        "\n",
        "    # Preparar dados para a tabela\n",
        "    table_data = filtered_df.to_dict('records')\n",
        "    table_columns = [{\"name\": i, \"id\": i} for i in filtered_df.columns]\n",
        "\n",
        "    return fig, table_data, table_columns\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXECUTAR A APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "# Para executar no Colab, use jupyter_mode=True\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True, jupyter_mode=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "imlrN1Z0J4Sx",
        "outputId": "740b9604-9637-485e-e2d3-38b24779b1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n",
            "\n",
            "--- A iniciar a preparação completa dos dados para o dashboard... ---\n",
            "--- A simular dados de exemplo... ---\n",
            "!! Lembre-se de substituir este bloco pelos seus dados reais. !!\n",
            "--- Engenharia de Atributos... ---\n",
            "--- Definindo a Complexidade... ---\n",
            "--- Treinando e prevendo com os modelos... ---\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "--- Consolidando resultados... ---\n",
            "Preparação dos dados concluída.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot convert '('f', 'i', 'l', 't', 'r', 'o', '-', 'a', 'n', 'o', 'm', 'a', 'l', 'i', 'a')' to a shape. Found invalid entry 'f' of type '<class 'str'>'. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2331010894.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data-table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data-table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filtro-anomalia'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filtro-complexidade'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, dtype, sparse, ragged, batch_shape, name, tensor, optional)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     layer = InputLayer(\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, batch_size, dtype, sparse, ragged, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36mstandardize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_int_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    608\u001b[0m                 \u001b[0;34mf\"Cannot convert '{shape}' to a shape. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;34mf\"Found invalid entry '{e}' of type '{type(e)}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot convert '('f', 'i', 'l', 't', 'r', 'o', '-', 'a', 'n', 'o', 'm', 'a', 'l', 'i', 'a')' to a shape. Found invalid entry 'f' of type '<class 'str'>'. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48e6f612",
        "outputId": "9af028dc-ff8f-4253-df55-409e014a1fb1"
      },
      "source": [
        "%pip install dash pandas numpy scikit-learn tensorflow plotly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dash\n",
            "  Downloading dash-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: Flask<3.2,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from dash) (3.1.1)\n",
            "Requirement already satisfied: Werkzeug<3.2 in /usr/local/lib/python3.11/dist-packages (from dash) (3.1.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash) (4.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash) (2.32.3)\n",
            "Collecting retrying (from dash)\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<3.2,>=1.0.4->dash) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.2,>=1.0.4->dash) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<3.2,>=1.0.4->dash) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.2,>=1.0.4->dash) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<3.2,>=1.0.4->dash) (3.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash) (3.23.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading dash-3.2.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: retrying, dash\n",
            "Successfully installed dash-3.2.0 retrying-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "50f26973",
        "outputId": "3a354fd7-a174-4f6d-ca5a-e8d2939c38c5"
      },
      "source": [
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "import dash\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Importações para o pipeline de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# (Esta função encapsula todo o processo que definimos anteriormente)\n",
        "# ==============================================================================\n",
        "\n",
        "def preparar_dados_para_dashboard():\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo: simulação de dados, engenharia de atributos,\n",
        "    modelagem e predição. Retorna um DataFrame final pronto para o dashboard.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A iniciar a preparação completa dos dados para o dashboard... ---\")\n",
        "\n",
        "    # --- Simulação de Dados (SUBSTITUA PELOS SEUS DADOS REAIS) ---\n",
        "    print(\"--- A simular dados de exemplo... ---\")\n",
        "    num_registos = 10000\n",
        "    data = {\n",
        "        'id_transacao': range(num_registos),\n",
        "        'data_hora': pd.to_datetime(np.random.choice(pd.date_range('2024-01-01', '2025-01-01', freq='h'), num_registos)),\n",
        "        'valor': np.random.lognormal(mean=8, sigma=1.5, size=num_registos).round(2),\n",
        "        'tipo_transacao': np.random.choice(['Pagamento', 'Recebimento', 'Transferência'], size=num_registos),\n",
        "        'categoria': np.random.choice(['Fornecedores', 'Salários', 'Marketing', 'Diversos', 'Outros'], size=num_registos, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
        "        'id_entidade': np.random.randint(1, 50, size=num_registos)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.loc[df.sample(frac=0.02).index, 'valor'] *= np.random.uniform(5, 10)\n",
        "    df.loc[df.sample(frac=0.01).index, 'valor'] *= -1\n",
        "    df.loc[df.sample(frac=0.01).index, 'data_hora'] += pd.to_timedelta(np.random.randint(21, 23), unit='h')\n",
        "    print(\"!! Lembre-se de substituir este bloco pelos seus dados reais. !!\")\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['categoria'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'])\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "\n",
        "    # Adicionar colunas originais necessárias para a lógica e o resultado final\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    # Isolation Forest\n",
        "    df_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "    iso_forest.fit(df_simples)\n",
        "    predicoes_if = iso_forest.predict(df_simples)\n",
        "\n",
        "    # Autoencoder\n",
        "    df_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "    else:\n",
        "        predicoes_ae = np.array([])\n",
        "\n",
        "\n",
        "    # --- Consolidação dos Resultados ---\n",
        "    print(\"--- Consolidando resultados... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    indices_simples = df_simples.index\n",
        "    df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "    df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    if not df_complexas.empty:\n",
        "        indices_complexas = df_complexas.index\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    print(\"Preparação dos dados concluída.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# Executar a função para obter o DataFrame final\n",
        "df_final = preparar_dados_para_dashboard()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "app = dash.Dash(__name__)\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = html.Div(style={'fontFamily': 'Arial, sans-serif', 'padding': '20px'}, children=[\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", style={'textAlign': 'center', 'color': '#003366'}),\n",
        "    html.H2(\"Caso de Estudo: System Business Development (SBD)\", style={'textAlign': 'center', 'color': '#505050'}),\n",
        "\n",
        "    html.Div(className='control-panel', style={'backgroundColor': '#f2f2f2', 'padding': '20px', 'borderRadius': '5px', 'marginBottom': '20px'}, children=[\n",
        "        html.H4(\"Filtros de Análise\", style={'marginTop': 0}),\n",
        "        html.Div(className='filters', style={'display': 'flex', 'gap': '40px'}, children=[\n",
        "            html.Div(children=[\n",
        "                html.Label(\"Estado da Anomalia:\"),\n",
        "                dcc.Dropdown(\n",
        "                    id='filtro-anomalia',\n",
        "                    options=[\n",
        "                        {'label': 'Todas', 'value': 'Todas'},\n",
        "                        {'label': 'Anómalas', 'value': 'Anómala'},\n",
        "                        {'label': 'Normais', 'value': 'Normal'}\n",
        "                    ],\n",
        "                    value='Todas'\n",
        "                )\n",
        "            ], style={'width': '33%'}),\n",
        "\n",
        "            html.Div(children=[\n",
        "                html.Label(\"Complexidade da Transação:\"),\n",
        "                dcc.Dropdown(\n",
        "                    id='filtro-complexidade',\n",
        "                    options=[\n",
        "                        {'label': 'Todas', 'value': 'Todas'},\n",
        "                        {'label': 'Simples', 'value': 'Simples'},\n",
        "                        {'label': 'Complexas', 'value': 'Complexa'}\n",
        "                    ],\n",
        "                    value='Todas'\n",
        "                )\n",
        "            ], style={'width': '33%'})\n",
        "        ])\n",
        "    ]),\n",
        "\n",
        "    # Gráfico de Dispersão\n",
        "    dcc.Graph(id='scatter-plot'),\n",
        "\n",
        "    # Tabela de Dados\n",
        "    html.Hr(),\n",
        "    html.H3(\"Detalhes das Transações Selecionadas\"),\n",
        "    dash_table.DataTable(\n",
        "        id='data-table',\n",
        "        columns=[{\"name\": i, \"id\": i} for i in df_final.columns],\n",
        "        page_size=10,\n",
        "        style_table={'overflowX': 'auto'},\n",
        "        style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "        style_cell={'textAlign': 'left', 'padding': '5px'},\n",
        "        sort_action=\"native\",\n",
        "        filter_action=\"native\",\n",
        "    )\n",
        "])\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. CALLBACKS - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "@app.callback(\n",
        "    [Output('scatter-plot', 'figure'),\n",
        "     Output('data-table', 'data'),\n",
        "     Output('data-table', 'columns')],\n",
        "    [DashInput('filtro-anomalia', 'value'), # Modificado aqui\n",
        "     DashInput('filtro-complexidade', 'value')] # E aqui\n",
        ")\n",
        "def update_dashboard(selected_anomalia, selected_complexidade):\n",
        "    \"\"\"\n",
        "    Atualiza o gráfico de dispersão e a tabela de dados com base nos filtros selecionados.\n",
        "    \"\"\"\n",
        "    filtered_df = df_final.copy()\n",
        "\n",
        "    # Aplicar filtro de anomalia\n",
        "    if selected_anomalia != 'Todas':\n",
        "        filtered_df = filtered_df[filtered_df['anomalia_detectada'] == selected_anomalia]\n",
        "\n",
        "    # Aplicar filtro de complexidade\n",
        "    if selected_complexidade != 'Todas':\n",
        "        filtered_df = filtered_df[filtered_df['complexidade'] == selected_complexidade]\n",
        "\n",
        "    # Criar gráfico de dispersão\n",
        "    fig = px.scatter(\n",
        "        filtered_df,\n",
        "        x=\"valor\",\n",
        "        y=\"desvio_media_historica\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['id_transacao', 'data_hora', 'tipo_transacao', 'categoria', 'id_entidade', 'complexidade', 'modelo_deteccao'],\n",
        "        title=\"Transações por Valor vs. Desvio da Média (Coloridas por Anomalia)\"\n",
        "    )\n",
        "\n",
        "    # Preparar dados para a tabela\n",
        "    table_data = filtered_df.to_dict('records')\n",
        "    table_columns = [{\"name\": i, \"id\": i} for i in filtered_df.columns]\n",
        "\n",
        "    return fig, table_data, table_columns\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXECUTAR A APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "# Para executar no Colab, use jupyter_mode=True\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True, jupyter_mode=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n",
            "\n",
            "--- A iniciar a preparação completa dos dados para o dashboard... ---\n",
            "--- A simular dados de exemplo... ---\n",
            "!! Lembre-se de substituir este bloco pelos seus dados reais. !!\n",
            "--- Engenharia de Atributos... ---\n",
            "--- Definindo a Complexidade... ---\n",
            "--- Treinando e prevendo com os modelos... ---\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "--- Consolidando resultados... ---\n",
            "Preparação dos dados concluída.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot convert '('f', 'i', 'l', 't', 'r', 'o', '-', 'a', 'n', 'o', 'm', 'a', 'l', 'i', 'a')' to a shape. Found invalid entry 'f' of type '<class 'str'>'. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1767411681.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m      \u001b[0mOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data-table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m      Output('data-table', 'columns')],\n\u001b[0;32m--> 195\u001b[0;31m     [Input('filtro-anomalia', 'value'),\n\u001b[0m\u001b[1;32m    196\u001b[0m      Input('filtro-complexidade', 'value')]\n\u001b[1;32m    197\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, dtype, sparse, ragged, batch_shape, name, tensor, optional)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     layer = InputLayer(\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, batch_size, dtype, sparse, ragged, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36mstandardize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_int_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    608\u001b[0m                 \u001b[0;34mf\"Cannot convert '{shape}' to a shape. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;34mf\"Found invalid entry '{e}' of type '{type(e)}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot convert '('f', 'i', 'l', 't', 'r', 'o', '-', 'a', 'n', 'o', 'm', 'a', 'l', 'i', 'a')' to a shape. Found invalid entry 'f' of type '<class 'str'>'. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP PARA GOOGLE COLAB - INSTALAÇÃO DE BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "# !pip install jupyter-dash -q # Jupyter-dash is not needed with newer Dash versions\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS (COM CORREÇÃO DE CONFLITO)\n",
        "# ==============================================================================\n",
        "import dash # Use dash directly\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Bibliotecas de Machine Learning e Pré-processamento\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Bibliotecas para o Autoencoder (Deep Learning)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def preparar_dados_para_dashboard():\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo: simulação de dados, engenharia de atributos,\n",
        "    modelagem e predição. Retorna um DataFrame final pronto para o dashboard.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A iniciar a preparação completa dos dados para o dashboard... ---\")\n",
        "\n",
        "    # --- Simulação de Dados (SUBSTITUA PELOS SEUS DADOS REAIS) ---\n",
        "    print(\"--- A simular dados de exemplo... ---\")\n",
        "    num_registos = 10000\n",
        "    data = {\n",
        "        'id_transacao': range(num_registos),\n",
        "        'data_hora': pd.to_datetime(np.random.choice(pd.date_range('2024-01-01', '2025-01-01', freq='h'), num_registos)),\n",
        "        'valor': np.random.lognormal(mean=8, sigma=1.5, size=num_registos).round(2),\n",
        "        'tipo_transacao': np.random.choice(['Pagamento', 'Recebimento', 'Transferência'], size=num_registos),\n",
        "        'categoria': np.random.choice(['Fornecedores', 'Salários', 'Marketing', 'Diversos', 'Outros'], size=num_registos, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
        "        'id_entidade': np.random.randint(1, 50, size=num_registos)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.loc[df.sample(frac=0.02).index, 'valor'] *= np.random.uniform(5, 10)\n",
        "    df.loc[df.sample(frac=0.01).index, 'valor'] *= -1\n",
        "    df.loc[df.sample(frac=0.01).index, 'data_hora'] += pd.to_timedelta(np.random.randint(21, 23), unit='h')\n",
        "    print(\"!! Lembre-se de substituir este bloco pelos seus dados reais. !!\")\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['categoria'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'])\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    # Isolation Forest\n",
        "    df_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "    iso_forest.fit(df_simples)\n",
        "    predicoes_if = iso_forest.predict(df_simples)\n",
        "\n",
        "    # Autoencoder\n",
        "    df_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "    else:\n",
        "        predicoes_ae = np.array([])\n",
        "\n",
        "    # --- Consolidação dos Resultados ---\n",
        "    print(\"--- Consolidando resultados... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    indices_simples = df_simples.index\n",
        "    df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "    df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    if not df_complexas.empty:\n",
        "        indices_complexas = df_complexas.index\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    print(\"Preparação dos dados concluída.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# Executar a função para obter o DataFrame final\n",
        "df_final = preparar_dados_para_dashboard()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "app = dash.Dash(__name__) # Use dash.Dash directly\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = html.Div(style={'fontFamily': 'Arial, sans-serif', 'padding': '20px'}, children=[\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", style={'textAlign': 'center', 'color': '#003366'}),\n",
        "    html.H2(\"Caso de Estudo: System Business Development (SBD)\", style={'textAlign': 'center', 'color': '#505050'}),\n",
        "\n",
        "    html.Div(className='control-panel', style={'backgroundColor': '#f2f2f2', 'padding': '20px', 'borderRadius': '5px', 'marginBottom': '20px'}, children=[\n",
        "        html.H4(\"Filtros de Análise\", style={'marginTop': 0}),\n",
        "        html.Div(className='filters', style={'display': 'flex', 'gap': '40px'}, children=[\n",
        "            html.Div(children=[\n",
        "                html.Label(\"Estado da Anomalia:\"),\n",
        "                dcc.Dropdown(\n",
        "                    id='filtro-anomalia',\n",
        "                    options=[\n",
        "                        {'label': 'Todas', 'value': 'Todas'},\n",
        "                        {'label': 'Anómalas', 'value': 'Anómala'},\n",
        "                        {'label': 'Normais', 'value': 'Normal'}\n",
        "                    ],\n",
        "                    value='Todas'\n",
        "                )\n",
        "            ], style={'width': '33%'}),\n",
        "\n",
        "            html.Div(children=[\n",
        "                html.Label(\"Complexidade da Transação:\"),\n",
        "                dcc.Dropdown(\n",
        "                    id='filtro-complexidade',\n",
        "                    options=[\n",
        "                        {'label': 'Todas', 'value': 'Todas'},\n",
        "                        {'label': 'Simples', 'value': 'Simples'},\n",
        "                        {'label': 'Complexas', 'value': 'Complexa'}\n",
        "                    ],\n",
        "                    value='Todas'\n",
        "                )\n",
        "            ], style={'width': '33%'})\n",
        "        ])\n",
        "    ]),\n",
        "\n",
        "    dcc.Graph(id='scatter-plot'),\n",
        "\n",
        "    html.Hr(),\n",
        "    html.H3(\"Detalhes das Transações Selecionadas\"),\n",
        "    dash_table.DataTable(\n",
        "        id='data-table',\n",
        "        columns=[{\"name\": i, \"id\": i} for i in df_final.columns],\n",
        "        page_size=10,\n",
        "        style_table={'overflowX': 'auto'},\n",
        "        style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "        style_cell={'textAlign': 'left', 'padding': '5px'},\n",
        "        sort_action=\"native\",\n",
        "        filter_action=\"native\",\n",
        "    )\n",
        "])\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. CALLBACKS - LÓGICA INTERATIVA DO DASHBOARD (COM CORREÇÃO)\n",
        "# ==============================================================================\n",
        "@app.callback(\n",
        "    [Output('scatter-plot', 'figure'),\n",
        "     Output('data-table', 'data'),\n",
        "     Output('data-table', 'columns')],\n",
        "    [DashInput('filtro-anomalia', 'value'),\n",
        "     DashInput('filtro-complexidade', 'value')]\n",
        ")\n",
        "def update_dashboard(status_anomalia, tipo_complexidade):\n",
        "    dff = df_final.copy()\n",
        "\n",
        "    if status_anomalia != 'Todas':\n",
        "        dff = dff[dff['anomalia_detectada'] == status_anomalia]\n",
        "\n",
        "    if tipo_complexidade != 'Todas':\n",
        "        dff = dff[dff['complexidade'] == tipo_complexidade]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        dff, x='data_hora', y='valor',\n",
        "        color='anomalia_detectada', size='valor',\n",
        "        hover_data=['id_transacao', 'categoria', 'complexidade', 'modelo_deteccao'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Transações Financeiras\",\n",
        "        template='plotly_white'\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500, xaxis_title=\"Data da Transação\", yaxis_title=\"Valor (MZN)\")\n",
        "\n",
        "    colunas_tabela = [{\"name\": i, \"id\": i} for i in dff.columns]\n",
        "    dados_tabela = dff.to_dict('records')\n",
        "\n",
        "    return fig, dados_tabela, colunas_tabela\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXECUÇÃO DA APLICAÇÃO NO COLAB\n",
        "# ==============================================================================\n",
        "if __name__ == '__main__':\n",
        "    app.run(mode='inline') # Use mode='inline' for Colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "w_aCDHRBdkIq",
        "outputId": "de17b31a-33d4-459b-a539-d334e4c77f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n",
            "\n",
            "--- A iniciar a preparação completa dos dados para o dashboard... ---\n",
            "--- A simular dados de exemplo... ---\n",
            "!! Lembre-se de substituir este bloco pelos seus dados reais. !!\n",
            "--- Engenharia de Atributos... ---\n",
            "--- Definindo a Complexidade... ---\n",
            "--- Treinando e prevendo com os modelos... ---\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "--- Consolidando resultados... ---\n",
            "Preparação dos dados concluída.\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:dash.dash:Dash is running on http://127.0.0.1:8050/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "run_simple() got an unexpected keyword argument 'mode'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3204648567.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use mode='inline' for Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dash/dash.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, host, port, proxy, debug, dev_tools_ui, dev_tools_props_check, dev_tools_serve_dev_bundles, dev_tools_hot_reload, dev_tools_hot_reload_interval, dev_tools_hot_reload_watch_interval, dev_tools_hot_reload_max_retry, dev_tools_silence_routes_logging, dev_tools_prune_errors, **flask_run_options)\u001b[0m\n\u001b[1;32m   1953\u001b[0m                     \u001b[0mextra_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflask_run_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menable_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flask/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, host, port, debug, load_dotenv, **options)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0mrun_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m             \u001b[0;31m# reset the first request information if the development server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: run_simple() got an unexpected keyword argument 'mode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP PARA GOOGLE COLAB - INSTALAÇÃO DE BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "!pip install jupyter-dash dash-bootstrap-components -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS (COM CORREÇÃO DE CONFLITO)\n",
        "# ==============================================================================\n",
        "from jupyter_dash import JupyterDash\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def preparar_dados_para_dashboard():\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo: simulação, engenharia de atributos, modelagem\n",
        "    e predição. Retorna um DataFrame final pronto para o dashboard.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A iniciar a preparação completa dos dados... ---\")\n",
        "\n",
        "    # --- Simulação de Dados (SUBSTITUA PELOS SEUS DADOS REAIS) ---\n",
        "    print(\"--- A simular dados de exemplo... ---\")\n",
        "    num_registos = 10000\n",
        "    data = {\n",
        "        'id_transacao': range(num_registos),\n",
        "        'data_hora': pd.to_datetime(np.random.choice(pd.date_range('2024-01-01', '2025-01-01', freq='h'), num_registos)),\n",
        "        'valor': np.random.lognormal(mean=8, sigma=1.5, size=num_registos).round(2),\n",
        "        'tipo_transacao': np.random.choice(['Pagamento', 'Recebimento', 'Transferência'], size=num_registos),\n",
        "        'categoria': np.random.choice(['Conta Corrente', 'Salários', 'Marketing', 'Diversos', 'Impostos'], size=num_registos, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
        "        'id_entidade': np.random.randint(1, 50, size=num_registos)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.loc[df.sample(frac=0.02).index, 'valor'] *= np.random.uniform(5, 10)\n",
        "    df.loc[df.sample(frac=0.01).index, 'valor'] *= -1\n",
        "    df.loc[df.sample(frac=0.01).index, 'data_hora'] += pd.to_timedelta(np.random.randint(21, 23), unit='h')\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    # Adicionando 'dia_do_ano' para o gráfico\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['categoria'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'])\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas.values - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    # Renomear 'categoria' para 'conta' para corresponder à interface do dashboard\n",
        "    df.rename(columns={'categoria': 'conta'}, inplace=True)\n",
        "\n",
        "    print(\"Preparação dos dados concluída.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. EXECUÇÃO DO PIPELINE E PREPARAÇÃO DOS DADOS FINAIS\n",
        "# ==============================================================================\n",
        "df_final = preparar_dados_para_dashboard()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH (VERSÃO INTEGRADA)\n",
        "# ==============================================================================\n",
        "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", className=\"my-4 text-center\"),\n",
        "    html.H2(\"Caso de Estudo: SBD (com Pipeline Otimizado)\", className=\"text-center text-muted mb-4\"),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            html.Label(\"Conta:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"conta-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + sorted(df_final[\"conta\"].unique())],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Tipo de Complexidade:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"tipo-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + df_final[\"complexidade\"].unique().tolist()],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Estado da Anomalia:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"anomalia-dropdown\",\n",
        "                options=[\n",
        "                    {\"label\": \"Todas\", \"value\": \"Todas\"},\n",
        "                    {\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"},\n",
        "                    {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}\n",
        "                ],\n",
        "                value=\"Todas\"\n",
        "            )\n",
        "        ], md=4)\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    dcc.Graph(id=\"scatter-graph\"),\n",
        "\n",
        "    dash_table.DataTable(\n",
        "        id=\"table\",\n",
        "        columns=[{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]],\n",
        "        page_size=10,\n",
        "        style_table={\"overflowX\": \"auto\"},\n",
        "        style_cell={\"textAlign\": \"left\"},\n",
        "        style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "        sort_action=\"native\",\n",
        "        filter_action=\"native\",\n",
        "    )\n",
        "], fluid=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. CALLBACK - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "@app.callback(\n",
        "    [Output(\"scatter-graph\", \"figure\"),\n",
        "     Output(\"table\", \"data\")],\n",
        "    [DashInput(\"conta-dropdown\", \"value\"),\n",
        "     DashInput(\"tipo-dropdown\", \"value\"),\n",
        "     DashInput(\"anomalia-dropdown\", \"value\")]\n",
        ")\n",
        "def actualizar_dashboard(conta, tipo, anomalia):\n",
        "    df_filtrado = df_final.copy()\n",
        "\n",
        "    if conta != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "\n",
        "    # Lógica de filtro para anomalias ajustada para os novos valores de texto\n",
        "    if anomalia == \"Anómalas\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia == \"Normais\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    # Criar o gráfico\n",
        "    fig = px.scatter(\n",
        "        df_filtrado, x=\"valor\", y=\"dia_do_ano\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['conta', 'complexidade', 'modelo_deteccao'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Anomalias (Valor vs. Dia do Ano)\",\n",
        "        labels={\"valor\": \"Valor da Transação (MZN)\", \"dia_do_ano\": \"Dia do Ano\"}\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500)\n",
        "\n",
        "    # Preparar dados para a tabela\n",
        "    data_tabela = df_filtrado[[\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]].to_dict(\"records\")\n",
        "\n",
        "    return fig, data_tabela\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. EXECUÇÃO DA APLICAÇÃO NO COLAB\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    app.run_server(mode='inline')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "4_LPXyWjiCqU",
        "outputId": "642008e6-9821-457c-f1a3-3d4b7dbcf919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/203.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBibliotecas importadas com sucesso.\n",
            "\n",
            "--- A iniciar a preparação completa dos dados... ---\n",
            "--- A simular dados de exemplo... ---\n",
            "--- Engenharia de Atributos... ---\n",
            "--- Definindo a Complexidade... ---\n",
            "--- Treinando e prevendo com os modelos... ---\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Preparação dos dados concluída.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dash/dash.py:634: UserWarning:\n",
            "\n",
            "JupyterDash is deprecated, use Dash instead.\n",
            "See https://dash.plotly.com/dash-in-jupyter for more details.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'super' object has no attribute 'run_server'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-745948675.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jupyter_dash/jupyter_app.py\u001b[0m in \u001b[0;36mrun_server\u001b[0;34m(self, mode, width, height, inline_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Get superclass run_server method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0msuper_run_server\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJupyterDash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mJupyterDash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_ipython\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute 'run_server'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP PARA GOOGLE COLAB - INSTALAÇÃO DE BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "!pip install dash dash-bootstrap-components plotly pandas scikit-learn tensorflow -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS (COM CORREÇÃO FINAL)\n",
        "# ==============================================================================\n",
        "from dash import Dash # CORREÇÃO: Usar a biblioteca Dash principal\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def preparar_dados_para_dashboard():\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo: simulação, engenharia de atributos, modelagem\n",
        "    e predição. Retorna um DataFrame final pronto para o dashboard.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A iniciar a preparação completa dos dados... ---\")\n",
        "\n",
        "    # --- Simulação de Dados (SUBSTITUA PELOS SEUS DADOS REAIS) ---\n",
        "    print(\"--- A simular dados de exemplo... ---\")\n",
        "    num_registos = 10000\n",
        "    data = {\n",
        "        'id_transacao': range(num_registos),\n",
        "        'data_hora': pd.to_datetime(np.random.choice(pd.date_range('2024-01-01', '2025-01-01', freq='h'), num_registos)),\n",
        "        'valor': np.random.lognormal(mean=8, sigma=1.5, size=num_registos).round(2),\n",
        "        'tipo_transacao': np.random.choice(['Pagamento', 'Recebimento', 'Transferência'], size=num_registos),\n",
        "        'categoria': np.random.choice(['Conta Corrente', 'Salários', 'Marketing', 'Diversos', 'Impostos'], size=num_registos, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
        "        'id_entidade': np.random.randint(1, 50, size=num_registos)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.loc[df.sample(frac=0.02).index, 'valor'] *= np.random.uniform(5, 10)\n",
        "    df.loc[df.sample(frac=0.01).index, 'valor'] *= -1\n",
        "    df.loc[df.sample(frac=0.01).index, 'data_hora'] += pd.to_timedelta(np.random.randint(21, 23), unit='h')\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['categoria'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'])\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas.values - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    # Renomear 'categoria' para 'conta' para corresponder à interface do dashboard\n",
        "    df.rename(columns={'categoria': 'conta'}, inplace=True)\n",
        "\n",
        "    print(\"Preparação dos dados concluída.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. EXECUÇÃO DO PIPELINE E PREPARAÇÃO DOS DADOS FINAIS\n",
        "# ==============================================================================\n",
        "df_final = preparar_dados_para_dashboard()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH (VERSÃO CORRIGIDA PARA COLAB)\n",
        "# ==============================================================================\n",
        "# CORREÇÃO: Usar Dash e especificar o modo para jupyter/colab na inicialização\n",
        "app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP], jupyter_mode=\"inline\")\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", className=\"my-4 text-center\"),\n",
        "    html.H2(\"Caso de Estudo: SBD (com Pipeline Otimizado)\", className=\"text-center text-muted mb-4\"),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            html.Label(\"Conta:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"conta-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + sorted(df_final[\"conta\"].unique())],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Tipo de Complexidade:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"tipo-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + df_final[\"complexidade\"].unique().tolist()],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Estado da Anomalia:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"anomalia-dropdown\",\n",
        "                options=[\n",
        "                    {\"label\": \"Todas\", \"value\": \"Todas\"},\n",
        "                    {\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"},\n",
        "                    {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}\n",
        "                ],\n",
        "                value=\"Todas\"\n",
        "            )\n",
        "        ], md=4)\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    dcc.Graph(id=\"scatter-graph\"),\n",
        "\n",
        "    dash_table.DataTable(\n",
        "        id=\"table\",\n",
        "        columns=[{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]],\n",
        "        page_size=10,\n",
        "        style_table={\"overflowX\": \"auto\"},\n",
        "        style_cell={\"textAlign\": \"left\"},\n",
        "        style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "        sort_action=\"native\",\n",
        "        filter_action=\"native\",\n",
        "    )\n",
        "], fluid=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. CALLBACK - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "@app.callback(\n",
        "    [Output(\"scatter-graph\", \"figure\"),\n",
        "     Output(\"table\", \"data\")],\n",
        "    [DashInput(\"conta-dropdown\", \"value\"),\n",
        "     DashInput(\"tipo-dropdown\", \"value\"),\n",
        "     DashInput(\"anomalia-dropdown\", \"value\")]\n",
        ")\n",
        "def actualizar_dashboard(conta, tipo, anomalia):\n",
        "    df_filtrado = df_final.copy()\n",
        "\n",
        "    if conta != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "\n",
        "    if anomalia == \"Anómalas\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia == \"Normais\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        df_filtrado, x=\"valor\", y=\"dia_do_ano\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['conta', 'complexidade', 'modelo_deteccao'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Anomalias (Valor vs. Dia do Ano)\",\n",
        "        labels={\"valor\": \"Valor da Transação (MZN)\", \"dia_do_ano\": \"Dia do Ano\"}\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500)\n",
        "\n",
        "    data_tabela = df_filtrado[[\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]].to_dict(\"records\")\n",
        "\n",
        "    return fig, data_tabela\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. EXECUÇÃO DA APLICAÇÃO NO COLAB (FORMA CORRETA)\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # CORREÇÃO: Usar app.run() para a nova versão do Dash\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "f1VdbohXlkvI",
        "outputId": "74c81888-e70b-4127-d20f-06ff560b8e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n",
            "\n",
            "--- A iniciar a preparação completa dos dados... ---\n",
            "--- A simular dados de exemplo... ---\n",
            "--- Engenharia de Atributos... ---\n",
            "--- Definindo a Complexidade... ---\n",
            "--- Treinando e prevendo com os modelos... ---\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Preparação dos dados concluída.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Dash() got an unexpected keyword argument 'jupyter_mode'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3548206958.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# CORREÇÃO: Usar Dash e especificar o modo para jupyter/colab na inicialização\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexternal_stylesheets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOOTSTRAP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Dashboard de Auditoria Financeira\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dash/dash.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, server, assets_folder, pages_folder, use_pages, assets_url_path, assets_ignore, assets_path_ignore, assets_external_path, eager_loading, include_assets_files, include_pages_meta, url_base_pathname, requests_pathname_prefix, routes_pathname_prefix, serve_locally, compress, meta_tags, index_string, external_scripts, external_stylesheets, suppress_callback_exceptions, prevent_initial_callbacks, show_undo_redo, extra_hot_reload_paths, plugins, title, update_title, background_callback_manager, add_log_handler, hooks, routing_callback_inputs, description, on_error, use_async, **obsolete)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 ) from exc\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0m_validate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_obsolete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobsolete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mcaller_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mget_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dash/_validate.py\u001b[0m in \u001b[0;36mcheck_obsolete\u001b[0;34m(kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# any other kwarg mimic the built-in exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dash() got an unexpected keyword argument '{key}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Dash() got an unexpected keyword argument 'jupyter_mode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP PARA GOOGLE COLAB - INSTALAÇÃO DE BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "!pip install dash dash-bootstrap-components plotly pandas scikit-learn tensorflow -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS (COM CORREÇÃO FINAL)\n",
        "# ==============================================================================\n",
        "from dash import Dash # CORREÇÃO: Usar a biblioteca Dash principal\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def preparar_dados_para_dashboard():\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo: simulação, engenharia de atributos, modelagem\n",
        "    e predição. Retorna um DataFrame final pronto para o dashboard.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A iniciar a preparação completa dos dados... ---\")\n",
        "\n",
        "    # --- Simulação de Dados (SUBSTITUA PELOS SEUS DADOS REAIS) ---\n",
        "    print(\"--- A simular dados de exemplo... ---\")\n",
        "    num_registos = 10000\n",
        "    data = {\n",
        "        'id_transacao': range(num_registos),\n",
        "        'data_hora': pd.to_datetime(np.random.choice(pd.date_range('2024-01-01', '2025-01-01', freq='h'), num_registos)),\n",
        "        'valor': np.random.lognormal(mean=8, sigma=1.5, size=num_registos).round(2),\n",
        "        'tipo_transacao': np.random.choice(['Pagamento', 'Recebimento', 'Transferência'], size=num_registos),\n",
        "        'categoria': np.random.choice(['Conta Corrente', 'Salários', 'Marketing', 'Diversos', 'Impostos'], size=num_registos, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
        "        'id_entidade': np.random.randint(1, 50, size=num_registos)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.loc[df.sample(frac=0.02).index, 'valor'] *= np.random.uniform(5, 10)\n",
        "    df.loc[df.sample(frac=0.01).index, 'valor'] *= -1\n",
        "    df.loc[df.sample(frac=0.01).index, 'data_hora'] += pd.to_timedelta(np.random.randint(21, 23), unit='h')\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['categoria'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'])\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas.values - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    # Renomear 'categoria' para 'conta' para corresponder à interface do dashboard\n",
        "    df.rename(columns={'categoria': 'conta'}, inplace=True)\n",
        "\n",
        "    print(\"Preparação dos dados concluída.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. EXECUÇÃO DO PIPELINE E PREPARAÇÃO DOS DADOS FINAIS\n",
        "# ==============================================================================\n",
        "df_final = preparar_dados_para_dashboard()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH (VERSÃO CORRIGIDA PARA COLAB)\n",
        "# ==============================================================================\n",
        "# CORREÇÃO: Usar Dash e especificar o modo para jupyter/colab na inicialização\n",
        "app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP], jupyter_mode=\"inline\")\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", className=\"my-4 text-center\"),\n",
        "    html.H2(\"Caso de Estudo: SBD (com Pipeline Otimizado)\", className=\"text-center text-muted mb-4\"),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            html.Label(\"Conta:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"conta-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + sorted(df_final[\"conta\"].unique())],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Tipo de Complexidade:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"tipo-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + df_final[\"complexidade\"].unique().tolist()],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Estado da Anomalia:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"anomalia-dropdown\",\n",
        "                options=[\n",
        "                    {\"label\": \"Todas\", \"value\": \"Todas\"},\n",
        "                    {\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"},\n",
        "                    {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}\n",
        "                ],\n",
        "                value=\"Todas\"\n",
        "            )\n",
        "        ], md=4)\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    dcc.Graph(id=\"scatter-graph\"),\n",
        "\n",
        "    dash_table.DataTable(\n",
        "        id=\"table\",\n",
        "        columns=[{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]],\n",
        "        page_size=10,\n",
        "        style_table={\"overflowX\": \"auto\"},\n",
        "        style_cell={\"textAlign\": \"left\"},\n",
        "        style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "        sort_action=\"native\",\n",
        "        filter_action=\"native\",\n",
        "    )\n",
        "], fluid=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. CALLBACK - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "@app.callback(\n",
        "    [Output(\"scatter-graph\", \"figure\"),\n",
        "     Output(\"table\", \"data\")],\n",
        "    [DashInput(\"conta-dropdown\", \"value\"),\n",
        "     DashInput(\"tipo-dropdown\", \"value\"),\n",
        "     DashInput(\"anomalia-dropdown\", \"value\")]\n",
        ")\n",
        "def actualizar_dashboard(conta, tipo, anomalia):\n",
        "    df_filtrado = df_final.copy()\n",
        "\n",
        "    if conta != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "\n",
        "    if anomalia == \"Anómalas\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia == \"Normais\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        df_filtrado, x=\"valor\", y=\"dia_do_ano\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['conta', 'complexidade', 'modelo_deteccao'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Anomalias (Valor vs. Dia do Ano)\",\n",
        "        labels={\"valor\": \"Valor da Transação (MZN)\", \"dia_do_ano\": \"Dia do Ano\"}\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500)\n",
        "\n",
        "    data_tabela = df_filtrado[[\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]].to_dict(\"records\")\n",
        "\n",
        "    return fig, data_tabela\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. EXECUÇÃO DA APLICAÇÃO NO COLAB (FORMA CORRETA)\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # CORREÇÃO: Usar app.run() para a nova versão do Dash\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "O15GznODl1NP",
        "outputId": "b692a1f0-968a-4ca4-f64b-93546d0b10d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n",
            "\n",
            "--- A iniciar a preparação completa dos dados... ---\n",
            "--- A simular dados de exemplo... ---\n",
            "--- Engenharia de Atributos... ---\n",
            "--- Definindo a Complexidade... ---\n",
            "--- Treinando e prevendo com os modelos... ---\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Preparação dos dados concluída.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Dash() got an unexpected keyword argument 'jupyter_mode'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3548206958.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# CORREÇÃO: Usar Dash e especificar o modo para jupyter/colab na inicialização\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexternal_stylesheets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOOTSTRAP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Dashboard de Auditoria Financeira\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dash/dash.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, server, assets_folder, pages_folder, use_pages, assets_url_path, assets_ignore, assets_path_ignore, assets_external_path, eager_loading, include_assets_files, include_pages_meta, url_base_pathname, requests_pathname_prefix, routes_pathname_prefix, serve_locally, compress, meta_tags, index_string, external_scripts, external_stylesheets, suppress_callback_exceptions, prevent_initial_callbacks, show_undo_redo, extra_hot_reload_paths, plugins, title, update_title, background_callback_manager, add_log_handler, hooks, routing_callback_inputs, description, on_error, use_async, **obsolete)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 ) from exc\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0m_validate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_obsolete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobsolete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mcaller_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mget_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dash/_validate.py\u001b[0m in \u001b[0;36mcheck_obsolete\u001b[0;34m(kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# any other kwarg mimic the built-in exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dash() got an unexpected keyword argument '{key}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Dash() got an unexpected keyword argument 'jupyter_mode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP PARA GOOGLE COLAB - INSTALAÇÃO E ATUALIZAÇÃO DE BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "# CORREÇÃO: Usar --upgrade para garantir que temos a versão mais recente do Dash (v2.0+)\n",
        "!pip install --upgrade dash dash-bootstrap-components plotly pandas scikit-learn tensorflow -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "from dash import Dash # Usar a biblioteca Dash principal\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def preparar_dados_para_dashboard():\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo: simulação, engenharia de atributos, modelagem\n",
        "    e predição. Retorna um DataFrame final pronto para o dashboard.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A iniciar a preparação completa dos dados... ---\")\n",
        "\n",
        "    # --- Simulação de Dados (SUBSTITUA PELOS SEUS DADOS REAIS) ---\n",
        "    print(\"--- A simular dados de exemplo... ---\")\n",
        "    num_registos = 10000\n",
        "    data = {\n",
        "        'id_transacao': range(num_registos),\n",
        "        'data_hora': pd.to_datetime(np.random.choice(pd.date_range('2024-01-01', '2025-01-01', freq='h'), num_registos)),\n",
        "        'valor': np.random.lognormal(mean=8, sigma=1.5, size=num_registos).round(2),\n",
        "        'tipo_transacao': np.random.choice(['Pagamento', 'Recebimento', 'Transferência'], size=num_registos),\n",
        "        'categoria': np.random.choice(['Conta Corrente', 'Salários', 'Marketing', 'Diversos', 'Impostos'], size=num_registos, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
        "        'id_entidade': np.random.randint(1, 50, size=num_registos)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.loc[df.sample(frac=0.02).index, 'valor'] *= np.random.uniform(5, 10)\n",
        "    df.loc[df.sample(frac=0.01).index, 'valor'] *= -1\n",
        "    df.loc[df.sample(frac=0.01).index, 'data_hora'] += pd.to_timedelta(np.random.randint(21, 23), unit='h')\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['categoria'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'])\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas.values - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    # Renomear 'categoria' para 'conta' para corresponder à interface do dashboard\n",
        "    df.rename(columns={'categoria': 'conta'}, inplace=True)\n",
        "\n",
        "    print(\"Preparação dos dados concluída.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. EXECUÇÃO DO PIPELINE E PREPARAÇÃO DOS DADOS FINAIS\n",
        "# ==============================================================================\n",
        "df_final = preparar_dados_para_dashboard()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP], jupyter_mode=\"inline\")\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", className=\"my-4 text-center\"),\n",
        "    html.H2(\"Caso de Estudo: SBD (com Pipeline Otimizado)\", className=\"text-center text-muted mb-4\"),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            html.Label(\"Conta:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"conta-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + sorted(df_final[\"conta\"].unique())],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Tipo de Complexidade:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"tipo-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + df_final[\"complexidade\"].unique().tolist()],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Estado da Anomalia:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"anomalia-dropdown\",\n",
        "                options=[\n",
        "                    {\"label\": \"Todas\", \"value\": \"Todas\"},\n",
        "                    {\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"},\n",
        "                    {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}\n",
        "                ],\n",
        "                value=\"Todas\"\n",
        "            )\n",
        "        ], md=4)\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    dcc.Graph(id=\"scatter-graph\"),\n",
        "\n",
        "    dash_table.DataTable(\n",
        "        id=\"table\",\n",
        "        columns=[{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]],\n",
        "        page_size=10,\n",
        "        style_table={\"overflowX\": \"auto\"},\n",
        "        style_cell={\"textAlign\": \"left\"},\n",
        "        style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "        sort_action=\"native\",\n",
        "        filter_action=\"native\",\n",
        "    )\n",
        "], fluid=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. CALLBACK - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "@app.callback(\n",
        "    [Output(\"scatter-graph\", \"figure\"),\n",
        "     Output(\"table\", \"data\")],\n",
        "    [DashInput(\"conta-dropdown\", \"value\"),\n",
        "     DashInput(\"tipo-dropdown\", \"value\"),\n",
        "     DashInput(\"anomalia-dropdown\", \"value\")]\n",
        ")\n",
        "def actualizar_dashboard(conta, tipo, anomalia):\n",
        "    df_filtrado = df_final.copy()\n",
        "\n",
        "    if conta != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "\n",
        "    if anomalia == \"Anómalas\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia == \"Normais\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        df_filtrado, x=\"valor\", y=\"dia_do_ano\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['conta', 'complexidade', 'modelo_deteccao'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Anomalias (Valor vs. Dia do Ano)\",\n",
        "        labels={\"valor\": \"Valor da Transação (MZN)\", \"dia_do_ano\": \"Dia do Ano\"}\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500)\n",
        "\n",
        "    data_tabela = df_filtrado[[\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]].to_dict(\"records\")\n",
        "\n",
        "    return fig, data_tabela\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. EXECUÇÃO DA APLICAÇÃO NO COLAB\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "tTnSn9Wnq9yt",
        "outputId": "a2ed3ce0-1184-4d2f-ac16-7794b858f910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mBibliotecas importadas com sucesso.\n",
            "\n",
            "--- A iniciar a preparação completa dos dados... ---\n",
            "--- A simular dados de exemplo... ---\n",
            "--- Engenharia de Atributos... ---\n",
            "--- Definindo a Complexidade... ---\n",
            "--- Treinando e prevendo com os modelos... ---\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Preparação dos dados concluída.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Dash() got an unexpected keyword argument 'jupyter_mode'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2164794637.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# 4. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexternal_stylesheets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOOTSTRAP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Dashboard de Auditoria Financeira\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dash/dash.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, server, assets_folder, pages_folder, use_pages, assets_url_path, assets_ignore, assets_path_ignore, assets_external_path, eager_loading, include_assets_files, include_pages_meta, url_base_pathname, requests_pathname_prefix, routes_pathname_prefix, serve_locally, compress, meta_tags, index_string, external_scripts, external_stylesheets, suppress_callback_exceptions, prevent_initial_callbacks, show_undo_redo, extra_hot_reload_paths, plugins, title, update_title, background_callback_manager, add_log_handler, hooks, routing_callback_inputs, description, on_error, use_async, **obsolete)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 ) from exc\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0m_validate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_obsolete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobsolete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mcaller_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mget_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dash/_validate.py\u001b[0m in \u001b[0;36mcheck_obsolete\u001b[0;34m(kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# any other kwarg mimic the built-in exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dash() got an unexpected keyword argument '{key}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Dash() got an unexpected keyword argument 'jupyter_mode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP - INSTALAÇÃO CUIDADOSA DE BIBLIOTECAS PARA O COLAB\n",
        "# ==============================================================================\n",
        "# CORREÇÃO: Pedimos uma versão do Dash >= 2.0 (para ter o jupyter_mode)\n",
        "# mas sem o --upgrade, para tentar respeitar as dependências do Colab.\n",
        "!pip install \"dash>=2.0.0\" dash-bootstrap-components plotly -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "from dash import Dash\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def preparar_dados_para_dashboard():\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo: simulação, engenharia de atributos, modelagem\n",
        "    e predição. Retorna um DataFrame final pronto para o dashboard.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A iniciar a preparação completa dos dados... ---\")\n",
        "\n",
        "    # --- Simulação de Dados (SUBSTITUA PELOS SEUS DADOS REAIS) ---\n",
        "    print(\"--- A simular dados de exemplo... ---\")\n",
        "    num_registos = 10000\n",
        "    data = {\n",
        "        'id_transacao': range(num_registos),\n",
        "        'data_hora': pd.to_datetime(np.random.choice(pd.date_range('2024-01-01', '2025-01-01', freq='h'), num_registos)),\n",
        "        'valor': np.random.lognormal(mean=8, sigma=1.5, size=num_registos).round(2),\n",
        "        'tipo_transacao': np.random.choice(['Pagamento', 'Recebimento', 'Transferência'], size=num_registos),\n",
        "        'categoria': np.random.choice(['Conta Corrente', 'Salários', 'Marketing', 'Diversos', 'Impostos'], size=num_registos, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
        "        'id_entidade': np.random.randint(1, 50, size=num_registos)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.loc[df.sample(frac=0.02).index, 'valor'] *= np.random.uniform(5, 10)\n",
        "    df.loc[df.sample(frac=0.01).index, 'valor'] *= -1\n",
        "    df.loc[df.sample(frac=0.01).index, 'data_hora'] += pd.to_timedelta(np.random.randint(21, 23), unit='h')\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['categoria'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'])\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas.values - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    # Renomear 'categoria' para 'conta' para corresponder à interface do dashboard\n",
        "    df.rename(columns={'categoria': 'conta'}, inplace=True)\n",
        "\n",
        "    print(\"Preparação dos dados concluída.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. EXECUÇÃO DO PIPELINE E PREPARAÇÃO DOS DADOS FINAIS\n",
        "# ==============================================================================\n",
        "df_final = preparar_dados_para_dashboard()\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "# CORREÇÃO: Removido jupyter_mode=\"inline\" daqui\n",
        "app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", className=\"my-4 text-center\"),\n",
        "    html.H2(\"Caso de Estudo: SBD Limitada\", className=\"text-center text-muted mb-4\"),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            html.Label(\"Conta:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"conta-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + sorted(df_final[\"conta\"].unique())],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Tipo de Complexidade:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"tipo-dropdown\",\n",
        "                options=[{\"label\": c, \"value\": c} for c in [\"Todos\"] + df_final[\"complexidade\"].unique().tolist()],\n",
        "                value=\"Todos\"\n",
        "            )\n",
        "        ], md=4),\n",
        "\n",
        "        dbc.Col([\n",
        "            html.Label(\"Estado da Anomalia:\"),\n",
        "            dcc.Dropdown(\n",
        "                id=\"anomalia-dropdown\",\n",
        "                options=[\n",
        "                    {\"label\": \"Todas\", \"value\": \"Todas\"},\n",
        "                    {\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"},\n",
        "                    {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}\n",
        "                ],\n",
        "                value=\"Todas\"\n",
        "            )\n",
        "        ], md=4)\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    dcc.Graph(id=\"scatter-graph\"),\n",
        "\n",
        "    dash_table.DataTable(\n",
        "        id=\"table\",\n",
        "        columns=[{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]],\n",
        "        page_size=10,\n",
        "        style_table={\"overflowX\": \"auto\"},\n",
        "        style_cell={\"textAlign\": \"left\"},\n",
        "        style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "        sort_action=\"native\",\n",
        "        filter_action=\"native\",\n",
        "    )\n",
        "], fluid=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. CALLBACK - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "@app.callback(\n",
        "    [Output(\"scatter-graph\", \"figure\"),\n",
        "     Output(\"table\", \"data\")],\n",
        "    [DashInput(\"conta-dropdown\", \"value\"),\n",
        "     DashInput(\"tipo-dropdown\", \"value\"),\n",
        "     DashInput(\"anomalia-dropdown\", \"value\")]\n",
        ")\n",
        "def actualizar_dashboard(conta, tipo, anomalia):\n",
        "    df_filtrado = df_final.copy()\n",
        "\n",
        "    if conta != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "\n",
        "    if anomalia == \"Anómalas\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia == \"Normais\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        df_filtrado, x=\"valor\", y=\"dia_do_ano\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['conta', 'complexidade', 'modelo_deteccao'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Anomalias (Valor vs. Dia do Ano)\",\n",
        "        labels={\"valor\": \"Valor da Transação (MZN)\", \"dia_do_ano\": \"Dia do Ano\"}\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500)\n",
        "\n",
        "    data_tabela = df_filtrado[[\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]].to_dict(\"records\")\n",
        "\n",
        "    return fig, data_tabela\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. EXECUÇÃO DA APLICAÇÃO NO COLAB\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # CORREÇÃO: Usar app.run_server com mode='inline'\n",
        "    app.run(mode='inline') # Use mode='inline' for Colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "whyjI8EE_iU6",
        "outputId": "7ba69906-5023-42d2-a40a-c14cde689ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n",
            "\n",
            "--- A iniciar a preparação completa dos dados... ---\n",
            "--- A simular dados de exemplo... ---\n",
            "--- Engenharia de Atributos... ---\n",
            "--- Definindo a Complexidade... ---\n",
            "--- Treinando e prevendo com os modelos... ---\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Preparação dos dados concluída.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP - INSTALAÇÃO DE BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "!pip install \"dash==2.10.2\" \"jupyter-dash==0.4.2\" dash-bootstrap-components plotly openpyxl -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "from jupyter_dash import JupyterDash\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output, State\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM (MODIFICADA)\n",
        "# ==============================================================================\n",
        "# MODIFICAÇÃO: A função agora recebe um DataFrame como argumento.\n",
        "def executar_pipeline_de_analise(df_inicial):\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo sobre um DataFrame fornecido.\n",
        "    Retorna um DataFrame final com os resultados da análise.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A executar o pipeline de análise sobre os novos dados... ---\")\n",
        "    df = df_inicial.copy()\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    # MODIFICAÇÃO: Usamos 'conta' que pode ser o nome da coluna no ficheiro do utilizador\n",
        "    criterio_2 = df['conta'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'], errors='ignore')\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas.values - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    print(\"Pipeline de análise concluído.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH (COM UPLOAD)\n",
        "# ==============================================================================\n",
        "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    # MODIFICAÇÃO: Componente para armazenar os dados processados em formato JSON\n",
        "    dcc.Store(id='dados-processados-memoria'),\n",
        "\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", className=\"my-4 text-center\"),\n",
        "\n",
        "    # MODIFICAÇÃO: Secção de Upload de Ficheiro\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            dcc.Upload(\n",
        "                id='upload-dados',\n",
        "                children=html.Div([\n",
        "                    'Arraste e Solte ou ',\n",
        "                    html.A('Selecione um Ficheiro')\n",
        "                ]),\n",
        "                style={\n",
        "                    'width': '100%', 'height': '60px', 'lineHeight': '60px',\n",
        "                    'borderWidth': '1px', 'borderStyle': 'dashed',\n",
        "                    'borderRadius': '5px', 'textAlign': 'center', 'margin': '10px'\n",
        "                },\n",
        "                multiple=False # Apenas um ficheiro de cada vez\n",
        "            ),\n",
        "            html.Div(id='output-upload-state') # Para mostrar o estado do upload\n",
        "        ])\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    html.Hr(),\n",
        "\n",
        "    # Painel de Filtros (igual ao anterior)\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            html.Label(\"Conta:\"),\n",
        "            dcc.Dropdown(id=\"conta-dropdown\", value=\"Todos\")\n",
        "        ], md=4),\n",
        "        dbc.Col([\n",
        "            html.Label(\"Tipo de Complexidade:\"),\n",
        "            dcc.Dropdown(id=\"tipo-dropdown\", value=\"Todos\")\n",
        "        ], md=4),\n",
        "        dbc.Col([\n",
        "            html.Label(\"Estado da Anomalia:\"),\n",
        "            dcc.Dropdown(id=\"anomalia-dropdown\", value=\"Todas\")\n",
        "        ], md=4)\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    dcc.Graph(id=\"scatter-graph\"),\n",
        "    dash_table.DataTable(id=\"table\", page_size=10, style_table={\"overflowX\": \"auto\"},\n",
        "                         style_cell={\"textAlign\": \"left\"}, style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "                         sort_action=\"native\", filter_action=\"native\")\n",
        "], fluid=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. CALLBACKS - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "\n",
        "# MODIFICAÇÃO: Novo callback para processar o ficheiro carregado\n",
        "@app.callback(\n",
        "    Output('dados-processados-memoria', 'data'),\n",
        "    Output('output-upload-state', 'children'),\n",
        "    Input('upload-dados', 'contents'),\n",
        "    State('upload-dados', 'filename')\n",
        ")\n",
        "def processar_ficheiro_carregado(contents, filename):\n",
        "    if contents is None:\n",
        "        return None, \"Por favor, carregue um ficheiro (CSV ou Excel) para iniciar a análise.\"\n",
        "\n",
        "    content_type, content_string = contents.split(',')\n",
        "    decoded = base64.b64decode(content_string)\n",
        "\n",
        "    try:\n",
        "        if 'csv' in filename:\n",
        "            # Assume que o utilizador carrega um ficheiro CSV\n",
        "            df_inicial = pd.read_csv(io.StringIO(decoded.decode('utf-8')))\n",
        "        elif 'xls' in filename or 'xlsx' in filename:\n",
        "            # Assume que o utilizador carrega um ficheiro Excel\n",
        "            df_inicial = pd.read_excel(io.BytesIO(decoded))\n",
        "        else:\n",
        "            return None, html.Div(['Tipo de ficheiro não suportado. Por favor, use CSV ou Excel.'], style={'color': 'red'})\n",
        "\n",
        "        # MODIFICAÇÃO: Renomear 'categoria' para 'conta' logo no início\n",
        "        if 'categoria' in df_inicial.columns:\n",
        "            df_inicial.rename(columns={'categoria': 'conta'}, inplace=True)\n",
        "\n",
        "        # Executar todo o pipeline de ML\n",
        "        df_processado = executar_pipeline_de_analise(df_inicial)\n",
        "\n",
        "        # Converter para JSON para armazenar no dcc.Store\n",
        "        json_data = df_processado.to_json(date_format='iso', orient='split')\n",
        "\n",
        "        return json_data, html.Div([f'Ficheiro \"{filename}\" carregado e processado com sucesso!'], style={'color': 'green'})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None, html.Div(['Ocorreu um erro ao processar o ficheiro.'], style={'color': 'red'})\n",
        "\n",
        "# MODIFICAÇÃO: O callback principal agora depende dos dados armazenados e atualiza os filtros\n",
        "@app.callback(\n",
        "    Output(\"scatter-graph\", \"figure\"),\n",
        "    Output(\"table\", \"data\"),\n",
        "    Output(\"table\", \"columns\"),\n",
        "    Output(\"conta-dropdown\", \"options\"),\n",
        "    Output(\"tipo-dropdown\", \"options\"),\n",
        "    Output(\"anomalia-dropdown\", \"options\"),\n",
        "    Input('dados-processados-memoria', 'data'),\n",
        "    Input(\"conta-dropdown\", \"value\"),\n",
        "    Input(\"tipo-dropdown\", \"value\"),\n",
        "    Input(\"anomalia-dropdown\", \"value\")\n",
        ")\n",
        "def actualizar_dashboard(json_data, conta, tipo, anomalia):\n",
        "    if json_data is None:\n",
        "        # Estado inicial antes de carregar um ficheiro\n",
        "        return {}, [], [], [], [], []\n",
        "\n",
        "    df_final = pd.read_json(json_data, orient='split')\n",
        "\n",
        "    # Atualizar opções dos filtros com base nos dados carregados\n",
        "    opcoes_conta = [{\"label\": c, \"value\": c} for c in [\"Todos\"] + sorted(df_final[\"conta\"].unique())]\n",
        "    opcoes_tipo = [{\"label\": c, \"value\": c} for c in [\"Todos\"] + df_final[\"complexidade\"].unique().tolist()]\n",
        "    opcoes_anomalia = [\n",
        "        {\"label\": \"Todas\", \"value\": \"Todas\"},\n",
        "        {\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"},\n",
        "        {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}\n",
        "    ]\n",
        "\n",
        "    # Lógica de filtragem\n",
        "    df_filtrado = df_final.copy()\n",
        "    if conta and conta != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo and tipo != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "    if anomalia and anomalia == \"Anómalas\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia and anomalia == \"Normais\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    # Criar o gráfico\n",
        "    fig = px.scatter(\n",
        "        df_filtrado, x=\"valor\", y=\"dia_do_ano\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['conta', 'complexidade', 'modelo_deteccao'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Anomalias (Valor vs. Dia do Ano)\",\n",
        "        labels={\"valor\": \"Valor da Transação (MZN)\", \"dia_do_ano\": \"Dia do Ano\"}\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500)\n",
        "\n",
        "    # Preparar dados para a tabela\n",
        "    colunas_tabela = [{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]]\n",
        "    data_tabela = df_filtrado[colunas_tabela_ids].to_dict(\"records\")\n",
        "\n",
        "    return fig, data_tabela, colunas_tabela, opcoes_conta, opcoes_tipo, opcoes_anomalia\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXECUÇÃO DA APLICAÇÃO NO COLAB\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    app.run_server(mode='inline')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "HYEFI5balOeJ",
        "outputId": "4a4d64d2-2e4c-4654-88ac-b6be16f5b7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/10.3 MB\u001b[0m \u001b[31m156.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m173.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/229.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBibliotecas importadas com sucesso.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot convert '('u', 'p', 'l', 'o', 'a', 'd', '-', 'd', 'a', 'd', 'o', 's')' to a shape. Found invalid entry 'u' of type '<class 'str'>'. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-499763523.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dados-processados-memoria'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output-upload-state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'children'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'upload-dados'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'contents'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'upload-dados'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'filename'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, dtype, sparse, ragged, batch_shape, name, tensor, optional)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     layer = InputLayer(\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, batch_size, dtype, sparse, ragged, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36mstandardize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_int_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    608\u001b[0m                 \u001b[0;34mf\"Cannot convert '{shape}' to a shape. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;34mf\"Found invalid entry '{e}' of type '{type(e)}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot convert '('u', 'p', 'l', 'o', 'a', 'd', '-', 'd', 'a', 'd', 'o', 's')' to a shape. Found invalid entry 'u' of type '<class 'str'>'. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP - INSTALAÇÃO DE VERSÕES ESPECÍFICAS E COMPATÍVEIS\n",
        "# ==============================================================================\n",
        "# Instalamos versões específicas que são conhecidas por funcionarem bem juntas.\n",
        "!pip install \"dash==2.10.2\" \"jupyter-dash==0.4.2\" dash-bootstrap-components plotly openpyxl -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS (COM CORREÇÃO FINAL)\n",
        "# ==============================================================================\n",
        "from jupyter_dash import JupyterDash\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "# CORREÇÃO: Garantir que Input e State vêm do Dash, e usar um alias para Input.\n",
        "from dash.dependencies import Input as DashInput, Output, State\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense # O Input do Keras mantém o nome original\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def executar_pipeline_de_analise(df_inicial):\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo sobre um DataFrame fornecido.\n",
        "    Retorna um DataFrame final com os resultados da análise.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A executar o pipeline de análise sobre os novos dados... ---\")\n",
        "    df = df_inicial.copy()\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    # Assumimos que a coluna 'conta' existe após o carregamento e renomeação\n",
        "    criterio_2 = df['conta'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'], errors='ignore')\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,)) # Este Input é do Keras\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6Z1yvr1vZeV",
        "outputId": "80d79506-b44f-4f69-8e77-6b04bfa648c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP - INSTALAÇÃO DE VERSÕES ESPECÍFICAS E COMPATÍVEIS\n",
        "# ==============================================================================\n",
        "# Instalamos versões específicas que são conhecidas por funcionarem bem juntas.\n",
        "!pip install \"dash==2.10.2\" \"jupyter-dash==0.4.2\" dash-bootstrap-components plotly openpyxl -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS (COM CORREÇÃO FINAL)\n",
        "# ==============================================================================\n",
        "from jupyter_dash import JupyterDash\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "# CORREÇÃO: Garantir que Input e State vêm do Dash, e usar um alias para Input.\n",
        "from dash.dependencies import Input as DashInput, Output, State\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense # O Input do Keras mantém o nome original\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def executar_pipeline_de_analise(df_inicial):\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo sobre um DataFrame fornecido.\n",
        "    Retorna um DataFrame final com os resultados da análise.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A executar o pipeline de análise sobre os novos dados... ---\")\n",
        "    df = df_inicial.copy()\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    # Assumimos que a coluna 'conta' existe após o carregamento e renomeação\n",
        "    criterio_2 = df['conta'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'], errors='ignore')\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,)) # Este Input é do Keras\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas.values - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    print(\"Pipeline de análise concluído.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    dcc.Store(id='dados-processados-memoria'),\n",
        "\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", className=\"my-4 text-center\"),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            dcc.Upload(\n",
        "                id='upload-dados',\n",
        "                children=html.Div(['Arraste e Solte ou ', html.A('Selecione um Ficheiro')]),\n",
        "                style={\n",
        "                    'width': '100%', 'height': '60px', 'lineHeight': '60px',\n",
        "                    'borderWidth': '1px', 'borderStyle': 'dashed',\n",
        "                    'borderRadius': '5px', 'textAlign': 'center', 'margin': '10px'\n",
        "                },\n",
        "                multiple=False\n",
        "            ),\n",
        "            html.Div(id='output-upload-state')\n",
        "        ])\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    html.Hr(),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([html.Label(\"Conta:\"), dcc.Dropdown(id=\"conta-dropdown\", value=\"Todos\")], md=4),\n",
        "        dbc.Col([html.Label(\"Tipo de Complexidade:\"), dcc.Dropdown(id=\"tipo-dropdown\", value=\"Todos\")], md=4),\n",
        "        dbc.Col([html.Label(\"Estado da Anomalia:\"), dcc.Dropdown(id=\"anomalia-dropdown\", value=\"Todas\")], md=4)\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    dcc.Graph(id=\"scatter-graph\"),\n",
        "    dash_table.DataTable(id=\"table\", page_size=10, style_table={\"overflowX\": \"auto\"},\n",
        "                         style_cell={\"textAlign\": \"left\"}, style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "                         sort_action=\"native\", filter_action=\"native\")\n",
        "], fluid=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. CALLBACKS - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "\n",
        "# Callback para processar o ficheiro carregado\n",
        "@app.callback(\n",
        "    Output('dados-processados-memoria', 'data'),\n",
        "    Output('output-upload-state', 'children'),\n",
        "    DashInput('upload-dados', 'contents'), # CORREÇÃO: Usar DashInput\n",
        "    State('upload-dados', 'filename')\n",
        ")\n",
        "def processar_ficheiro_carregado(contents, filename):\n",
        "    if contents is None:\n",
        "        return None, \"Por favor, carregue um ficheiro (CSV ou Excel) para iniciar a análise.\"\n",
        "\n",
        "    content_type, content_string = contents.split(',')\n",
        "    decoded = base64.b64decode(content_string)\n",
        "\n",
        "    try:\n",
        "        if 'csv' in filename:\n",
        "            df_inicial = pd.read_csv(io.StringIO(decoded.decode('utf-8')))\n",
        "        elif 'xls' in filename or 'xlsx' in filename:\n",
        "            # Se o ficheiro Excel tiver lixo nas primeiras linhas, pode usar 'skiprows'\n",
        "            # Ex: df_inicial = pd.read_excel(io.BytesIO(decoded), skiprows=3) para ignorar as 3 primeiras linhas\n",
        "            df_inicial = pd.read_excel(io.BytesIO(decoded))\n",
        "        else:\n",
        "            return None, html.Div(['Tipo de ficheiro não suportado. Por favor, use CSV ou Excel.'], style={'color': 'red'})\n",
        "\n",
        "        # --- INÍCIO DO BLOCO DE LIMPEZA AUTOMÁTICA ---\n",
        "\n",
        "        # 1. Renomear colunas (exemplo: se o seu ficheiro tiver \"Montante\" em vez de \"valor\")\n",
        "        colunas_para_renomear = {\n",
        "            'ID da Transação': 'id_transacao',\n",
        "            'Data': 'data_hora',\n",
        "            'Montante': 'valor',\n",
        "            'Categoria da Conta': 'conta',\n",
        "            'Entidade': 'id_entidade',\n",
        "            'Tipo': 'tipo_transacao'\n",
        "        }\n",
        "        # Renomeia apenas as colunas que existem no dicionário\n",
        "        df_inicial.rename(columns=colunas_para_renomear, inplace=True)\n",
        "\n",
        "        # 2. Converter colunas para os tipos corretos\n",
        "        # Converte 'data_hora' para datetime, tratando possíveis erros\n",
        "        df_inicial['data_hora'] = pd.to_datetime(df_inicial['data_hora'], errors='coerce')\n",
        "\n",
        "        # Converte 'valor' para número, transformando o que não for número em Nulo (NaN)\n",
        "        df_inicial['valor'] = pd.to_numeric(df_inicial['valor'], errors='coerce')\n",
        "\n",
        "        # 3. Tratar valores em falta (exemplo: apagar linhas onde 'valor' ou 'data_hora' são nulos)\n",
        "        df_inicial.dropna(subset=['valor', 'data_hora'], inplace=True)\n",
        "\n",
        "        # --- FIM DO BLOCO DE LIMPEZA ---\n",
        "\n",
        "        # Agora o df_inicial está pronto para o pipeline\n",
        "        df_processado = executar_pipeline_de_analise(df_inicial)\n",
        "\n",
        "        # Converter para JSON para armazenar no dcc.Store\n",
        "        json_data = df_processado.to_json(date_format='iso', orient='split')\n",
        "\n",
        "        return json_data, html.Div([f'Ficheiro \"{filename}\" carregado e processado com sucesso!'], style={'color': 'green'})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None, html.Div(['Ocorreu um erro ao processar o ficheiro. Verifique se as colunas estão corretas (ex: \"conta\", \"valor\", \"data_hora\").'], style={'color': 'red'})\n",
        "\n",
        "# Callback principal que atualiza o dashboard com base nos dados processados\n",
        "@app.callback(\n",
        "    Output(\"scatter-graph\", \"figure\"),\n",
        "    Output(\"table\", \"data\"),\n",
        "    Output(\"table\", \"columns\"),\n",
        "    Output(\"conta-dropdown\", \"options\"),\n",
        "    Output(\"tipo-dropdown\", \"options\"),\n",
        "    Output(\"anomalia-dropdown\", \"options\"),\n",
        "    DashInput('dados-processados-memoria', 'data'), # CORREÇÃO: Usar DashInput\n",
        "    DashInput(\"conta-dropdown\", \"value\"),\n",
        "    DashInput(\"tipo-dropdown\", \"value\"),\n",
        "    DashInput(\"anomalia-dropdown\", \"value\")\n",
        ")\n",
        "def actualizar_dashboard(json_data, conta, tipo, anomalia):\n",
        "    # Estado inicial antes de carregar um ficheiro ou se ocorrer um erro\n",
        "    if json_data is None:\n",
        "        fig_vazia = {\"layout\": {\"xaxis\": {\"visible\": False}, \"yaxis\": {\"visible\": False}, \"annotations\": [{\"text\": \"Nenhum dado para exibir. Por favor, carregue um ficheiro.\", \"xref\": \"paper\", \"yref\": \"paper\", \"showarrow\": False, \"font\": {\"size\": 20}}]}}\n",
        "        colunas_vazias = [{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]]\n",
        "        opcoes_vazias = [{\"label\": \"Todos\", \"value\": \"Todos\"}]\n",
        "        opcoes_anomalia_fixas = [\n",
        "            {\"label\": \"Todas\", \"value\": \"Todas\"},\n",
        "            {\"label\": \"Apenas Anómalas\", \"value\": \"Anómalas\"},\n",
        "            {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}\n",
        "        ]\n",
        "        return fig_vazia, [], colunas_vazias, opcoes_vazias, opcoes_vazias, opcoes_anomalia_fixas\n",
        "\n",
        "    df_final = pd.read_json(json_data, orient='split')\n",
        "    df_final['data_hora'] = pd.to_datetime(df_final['data_hora'])\n",
        "\n",
        "    # Atualizar opções dos filtros\n",
        "    opcoes_conta = [{\"label\": c, \"value\": c} for c in [\"Todos\"] + sorted(df_final[\"conta\"].unique())]\n",
        "    opcoes_tipo = [{\"label\": c, \"value\": c} for c in [\"Todos\"] + df_final[\"complexidade\"].unique().tolist()]\n",
        "    opcoes_anomalia = [\n",
        "        {\"label\": \"Todas\", \"value\": \"Todas\"},\n",
        "        {\"label\": \"Apenas Anómalas\", \"value\": \"Anómalas\"},\n",
        "        {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}\n",
        "    ]\n",
        "\n",
        "    # Lógica de filtragem\n",
        "    df_filtrado = df_final.copy()\n",
        "    if conta and conta != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo and tipo != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "    if anomalia and anomalia == \"Anómalas\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia and anomalia == \"Normais\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    # Criar o gráfico\n",
        "    fig = px.scatter(\n",
        "        df_filtrado, x=\"valor\", y=\"dia_do_ano\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['conta', 'complexidade', 'modelo_deteccao', 'data_hora'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Anomalias (Valor vs. Dia do Ano)\",\n",
        "        labels={\"valor\": \"Valor da Transação (MZN)\", \"dia_do_ano\": \"Dia do Ano\"}\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500)\n",
        "\n",
        "    # Preparar dados para a tabela\n",
        "    colunas_tabela = [{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]]\n",
        "    data_tabela = df_filtrado[ [c['id'] for c in colunas_tabela] ].to_dict(\"records\")\n",
        "\n",
        "    return fig, data_tabela, colunas_tabela, opcoes_conta, opcoes_tipo, opcoes_anomalia\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXECUÇÃO DA APLICAÇÃO NO COLAB\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    app.run_server(mode='inline')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "nd2r6SzdvhW3",
        "outputId": "4d588eab-74ee-4f99-e148-e54ab4e386bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:dash.dash:Dash is running on http://127.0.0.1:8050/\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **----------------------------ALGORITMO FUNCIONAL---------------------**"
      ],
      "metadata": {
        "id": "2FdXpAAmmxiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP - INSTALAÇÃO DE VERSÕES ESPECÍFICAS E COMPATÍVEIS\n",
        "# ==============================================================================\n",
        "# Instalamos versões específicas que são conhecidas por funcionarem bem juntas.\n",
        "!pip install \"dash==2.10.2\" \"jupyter-dash==0.4.2\" dash-bootstrap-components plotly openpyxl -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "from jupyter_dash import JupyterDash\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output, State\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def executar_pipeline_de_analise(df_inicial):\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo sobre um DataFrame fornecido.\n",
        "    Retorna um DataFrame final com os resultados da análise.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A executar o pipeline de análise sobre os novos dados... ---\")\n",
        "    df = df_inicial.copy()\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['conta'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'], errors='ignore')\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas.values - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    print(\"Pipeline de análise concluído.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    dcc.Store(id='dados-processados-memoria'),\n",
        "\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", className=\"my-4 text-center\"),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            dcc.Upload(\n",
        "                id='upload-dados',\n",
        "                children=html.Div(['Arraste e Solte ou ', html.A('Selecione um Ficheiro')]),\n",
        "                style={\n",
        "                    'width': '100%', 'height': '60px', 'lineHeight': '60px',\n",
        "                    'borderWidth': '1px', 'borderStyle': 'dashed',\n",
        "                    'borderRadius': '5px', 'textAlign': 'center', 'margin': '10px'\n",
        "                },\n",
        "                multiple=False\n",
        "            ),\n",
        "            html.Div(id='output-upload-state')\n",
        "        ])\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    html.Hr(),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([html.Label(\"Conta:\"), dcc.Dropdown(id=\"conta-dropdown\", value=\"Todos\")], md=4),\n",
        "        dbc.Col([html.Label(\"Tipo de Complexidade:\"), dcc.Dropdown(id=\"tipo-dropdown\", value=\"Todos\")], md=4),\n",
        "        dbc.Col([html.Label(\"Estado da Anomalia:\"), dcc.Dropdown(id=\"anomalia-dropdown\", value=\"Todas\")], md=4)\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    dcc.Graph(id=\"scatter-graph\"),\n",
        "    dash_table.DataTable(id=\"table\", page_size=10, style_table={\"overflowX\": \"auto\"},\n",
        "                         style_cell={\"textAlign\": \"left\"}, style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "                         sort_action=\"native\", filter_action=\"native\")\n",
        "], fluid=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. CALLBACKS - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "\n",
        "# Callback para processar o ficheiro carregado\n",
        "@app.callback(\n",
        "    Output('dados-processados-memoria', 'data'),\n",
        "    Output('output-upload-state', 'children'),\n",
        "    DashInput('upload-dados', 'contents'),\n",
        "    State('upload-dados', 'filename')\n",
        ")\n",
        "def processar_ficheiro_carregado(contents, filename):\n",
        "    if contents is None:\n",
        "        return None, \"Por favor, carregue um ficheiro (CSV ou Excel) para iniciar a análise.\"\n",
        "\n",
        "    content_type, content_string = contents.split(',')\n",
        "    decoded = base64.b64decode(content_string)\n",
        "\n",
        "    try:\n",
        "        if 'csv' in filename:\n",
        "            df_inicial = pd.read_csv(io.StringIO(decoded.decode('utf-8')))\n",
        "        elif 'xls' in filename or 'xlsx' in filename:\n",
        "            df_inicial = pd.read_excel(io.BytesIO(decoded))\n",
        "        else:\n",
        "            return None, html.Div(['Tipo de ficheiro não suportado. Por favor, use CSV ou Excel.'], style={'color': 'red'})\n",
        "\n",
        "        # --- INÍCIO DO BLOCO DE LIMPEZA AUTOMÁTICA ---\n",
        "\n",
        "        # !! IMPORTANTE: AJUSTE ESTE DICIONÁRIO PARA OS NOMES DAS SUAS COLUNAS !!\n",
        "        colunas_para_renomear = {\n",
        "            'ID da Transação': 'id_transacao',\n",
        "            'Data': 'data_hora',\n",
        "            'Montante': 'valor',\n",
        "            'Categoria da Conta': 'conta',\n",
        "            'Entidade': 'id_entidade',\n",
        "            'Tipo': 'tipo_transacao'\n",
        "        }\n",
        "        df_inicial.rename(columns=colunas_para_renomear, inplace=True)\n",
        "\n",
        "        # Validação das colunas essenciais\n",
        "        colunas_necessarias = ['id_transacao', 'data_hora', 'valor', 'conta', 'id_entidade']\n",
        "        colunas_em_falta = [col for col in colunas_necessarias if col not in df_inicial.columns]\n",
        "        if colunas_em_falta:\n",
        "            return None, html.Div([f'Erro: Faltam as seguintes colunas no ficheiro: {\", \".join(colunas_em_falta)}'], style={'color': 'red'})\n",
        "\n",
        "        df_inicial['data_hora'] = pd.to_datetime(df_inicial['data_hora'], errors='coerce')\n",
        "        df_inicial['valor'] = pd.to_numeric(df_inicial['valor'], errors='coerce')\n",
        "        df_inicial.dropna(subset=['valor', 'data_hora'], inplace=True)\n",
        "\n",
        "        # --- FIM DO BLOCO DE LIMPEZA ---\n",
        "\n",
        "        df_processado = executar_pipeline_de_analise(df_inicial)\n",
        "        json_data = df_processado.to_json(date_format='iso', orient='split')\n",
        "\n",
        "        return json_data, html.Div([f'Ficheiro \"{filename}\" carregado e processado com sucesso!'], style={'color': 'green'})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None, html.Div(['Ocorreu um erro ao processar o ficheiro. Verifique o formato e as colunas.'], style={'color': 'red'})\n",
        "\n",
        "# Callback principal que atualiza o dashboard\n",
        "@app.callback(\n",
        "    Output(\"scatter-graph\", \"figure\"),\n",
        "    Output(\"table\", \"data\"),\n",
        "    Output(\"table\", \"columns\"),\n",
        "    Output(\"conta-dropdown\", \"options\"),\n",
        "    Output(\"tipo-dropdown\", \"options\"),\n",
        "    Output(\"anomalia-dropdown\", \"options\"),\n",
        "    DashInput('dados-processados-memoria', 'data'),\n",
        "    DashInput(\"conta-dropdown\", \"value\"),\n",
        "    DashInput(\"tipo-dropdown\", \"value\"),\n",
        "    DashInput(\"anomalia-dropdown\", \"value\")\n",
        ")\n",
        "def actualizar_dashboard(json_data, conta, tipo, anomalia):\n",
        "    if json_data is None:\n",
        "        fig_vazia = {\"layout\": {\"xaxis\": {\"visible\": False}, \"yaxis\": {\"visible\": False}, \"annotations\": [{\"text\": \"Nenhum dado para exibir. Carregue um ficheiro.\", \"xref\": \"paper\", \"yref\": \"paper\", \"showarrow\": False, \"font\": {\"size\": 20}}]}}\n",
        "        colunas_vazias = [{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]]\n",
        "        opcoes_vazias = [{\"label\": \"Todos\", \"value\": \"Todos\"}]\n",
        "        opcoes_anomalia_fixas = [{\"label\": \"Todas\", \"value\": \"Todas\"}, {\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"}, {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}]\n",
        "        return fig_vazia, [], colunas_vazias, opcoes_vazias, opcoes_vazias, opcoes_anomalia_fixas\n",
        "\n",
        "    df_final = pd.read_json(json_data, orient='split')\n",
        "    df_final['data_hora'] = pd.to_datetime(df_final['data_hora'])\n",
        "\n",
        "    opcoes_conta = [{\"label\": c, \"value\": c} for c in [\"Todos\"] + sorted(df_final[\"conta\"].unique())]\n",
        "    opcoes_tipo = [{\"label\": c, \"value\": c} for c in [\"Todos\"] + df_final[\"complexidade\"].unique().tolist()]\n",
        "    opcoes_anomalia = [{\"label\": \"Todas\", \"value\": \"Todas\"}, {\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"}, {\"label\": \"Apenas Normais\", \"value\": \"Normais\"}]\n",
        "\n",
        "    df_filtrado = df_final.copy()\n",
        "    if conta and conta != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo and tipo != \"Todos\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "    if anomalia and anomalia == \"Anómalas\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia and anomalia == \"Normais\":\n",
        "        df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        df_filtrado, x=\"valor\", y=\"dia_do_ano\",\n",
        "        color=\"anomalia_detectada\",\n",
        "        hover_data=['conta', 'complexidade', 'modelo_deteccao', 'data_hora'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Anomalias (Valor vs. Dia do Ano)\",\n",
        "        labels={\"valor\": \"Valor da Transação (MZN)\", \"dia_do_ano\": \"Dia do Ano\"}\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500)\n",
        "\n",
        "    colunas_tabela = [{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]]\n",
        "    data_tabela = df_filtrado[[c['id'] for c in colunas_tabela]].to_dict(\"records\")\n",
        "\n",
        "    return fig, data_tabela, colunas_tabela, opcoes_conta, opcoes_tipo, opcoes_anomalia\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXECUÇÃO DA APLICAÇÃO NO COLAB\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    app.run_server(mode='inline')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "id": "LzDuU9op6rLA",
        "outputId": "5f34840e-dba7-4f18-8b9e-240c68423b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBibliotecas importadas com sucesso.\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:dash.dash:Dash is running on http://127.0.0.1:8050/\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **---------------ALGORITMO FUNCIONAL---------** ⏫::::"
      ],
      "metadata": {
        "id": "enjaU25GnH7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **-----------------------ALGORITMO FUNCIONAL C/ FUNCIONALIDADE DE BAIXAR RELATORIO------------------------------------------------------------**:  ⬇ ⏬"
      ],
      "metadata": {
        "id": "07jiRVWBrkW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. SETUP - INSTALAÇÃO DE VERSÕES ESPECÍFICAS E COMPATÍVEIS\n",
        "# ==============================================================================\n",
        "# Instalamos versões específicas que são conhecidas por funcionarem bem juntas.\n",
        "!pip install \"dash==2.10.2\" \"jupyter-dash==0.4.2\" dash-bootstrap-components plotly openpyxl -q\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP - IMPORTAÇÃO DAS BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "from jupyter_dash import JupyterDash\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input as DashInput, Output, State\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "\n",
        "# Bibliotecas de Machine Learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNÇÃO DO PIPELINE DE DADOS E MODELAGEM\n",
        "# ==============================================================================\n",
        "def executar_pipeline_de_analise(df_inicial):\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo sobre um DataFrame fornecido.\n",
        "    Retorna um DataFrame final com os resultados da análise.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- A executar o pipeline de análise sobre os novos dados... ---\")\n",
        "    df = df_inicial.copy()\n",
        "\n",
        "    # --- Engenharia de Atributos ---\n",
        "    print(\"--- Engenharia de Atributos... ---\")\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    df['dia_do_ano'] = df['data_hora'].dt.dayofyear\n",
        "    mediana_por_entidade = df.groupby('id_entidade')['valor'].transform('median')\n",
        "    df['desvio_mediana_historica'] = (df['valor'] - mediana_por_entidade) / (mediana_por_entidade + 1e-6)\n",
        "    df['hora_incomum'] = ((df['data_hora'].dt.hour < 7) | (df['data_hora'].dt.hour > 20)).astype(int)\n",
        "    df['dia_fim_de_semana'] = (df['data_hora'].dt.dayofweek >= 5).astype(int)\n",
        "    df['arredondamento_valor'] = (df['valor'] > 1000) & (df['valor'] % 1000 == 0).astype(int)\n",
        "    df = df.sort_values(by=['id_entidade', 'data_hora'])\n",
        "    diferenca_tempo = df.groupby('id_entidade')['data_hora'].diff().dt.total_seconds()\n",
        "    df['sequencia_rapida'] = (diferenca_tempo < 60).astype(int)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # --- Definição da Complexidade ---\n",
        "    print(\"--- Definindo a Complexidade... ---\")\n",
        "    media_geral = df['valor'].mean()\n",
        "    std_geral = df['valor'].std()\n",
        "    criterio_1 = df['valor'] > (media_geral + 3 * std_geral)\n",
        "    criterio_2 = df['conta'].isin(['Diversos', 'Outros'])\n",
        "    criterio_3 = df['valor'] < 0\n",
        "    df['complexidade'] = np.where((criterio_1 | criterio_2 | criterio_3), 'Complexa', 'Simples')\n",
        "\n",
        "    # --- Pré-processamento e Divisão ---\n",
        "    features_para_modelo = df.select_dtypes(include=np.number).columns.drop(['id_transacao', 'id_entidade'], errors='ignore')\n",
        "    X = df[features_para_modelo]\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=features_para_modelo, index=df.index)\n",
        "    df_scaled[['complexidade', 'id_transacao']] = df[['complexidade', 'id_transacao']]\n",
        "\n",
        "    # --- Treino e Predição ---\n",
        "    print(\"--- Treinando e prevendo com os modelos... ---\")\n",
        "    df['anomalia_detectada'] = 'Normal'\n",
        "    df['modelo_deteccao'] = 'N/A'\n",
        "\n",
        "    # Isolation Forest\n",
        "    indices_simples = df_scaled[df_scaled['complexidade'] == 'Simples'].index\n",
        "    df_simples = df_scaled.loc[indices_simples].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_simples.empty:\n",
        "        iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42, n_jobs=-1)\n",
        "        iso_forest.fit(df_simples)\n",
        "        predicoes_if = iso_forest.predict(df_simples)\n",
        "        df.loc[indices_simples, 'anomalia_detectada'] = np.where(predicoes_if == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_simples, 'modelo_deteccao'] = 'Isolation Forest'\n",
        "\n",
        "    # Autoencoder\n",
        "    indices_complexas = df_scaled[df_scaled['complexidade'] == 'Complexa'].index\n",
        "    df_complexas = df_scaled.loc[indices_complexas].drop(['complexidade', 'id_transacao'], axis=1)\n",
        "    if not df_complexas.empty:\n",
        "        input_dim = df_complexas.shape[1]\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        encoder = Dense(16, activation='relu')(input_layer)\n",
        "        encoder = Dense(8, activation='relu')(encoder)\n",
        "        decoder = Dense(16, activation='relu')(encoder)\n",
        "        decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "        autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        autoencoder.fit(df_complexas, df_complexas, epochs=50, batch_size=32, verbose=0)\n",
        "        reconstrucoes = autoencoder.predict(df_complexas)\n",
        "        mse = np.mean(np.power(df_complexas.values - reconstrucoes, 2), axis=1)\n",
        "        threshold = np.quantile(mse, 0.95)\n",
        "        predicoes_ae = np.where(mse > threshold, -1, 1)\n",
        "        df.loc[indices_complexas, 'anomalia_detectada'] = np.where(predicoes_ae == -1, 'Anómala', 'Normal')\n",
        "        df.loc[indices_complexas, 'modelo_deteccao'] = 'Autoencoder'\n",
        "\n",
        "    print(\"Pipeline de análise concluído.\")\n",
        "    return df.sort_index()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. INICIALIZAÇÃO E LAYOUT DA APLICAÇÃO DASH\n",
        "# ==============================================================================\n",
        "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "app.title = \"Dashboard de Auditoria Financeira\"\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    dcc.Store(id='dados-processados-memoria'),\n",
        "    dcc.Download(id=\"download-relatorio-csv\"),\n",
        "\n",
        "    html.H1(\"Dashboard de Auditoria de Anomalias Financeiras\", className=\"my-4 text-center\"),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([\n",
        "            dcc.Upload(\n",
        "                id='upload-dados',\n",
        "                children=html.Div(['Arraste e Solte ou ', html.A('Selecione um Ficheiro')]),\n",
        "                style={\n",
        "                    'width': '100%', 'height': '60px', 'lineHeight': '60px',\n",
        "                    'borderWidth': '1px', 'borderStyle': 'dashed',\n",
        "                    'borderRadius': '5px', 'textAlign': 'center', 'margin': '10px'\n",
        "                },\n",
        "                multiple=False\n",
        "            ),\n",
        "            html.Div(id='output-upload-state')\n",
        "        ])\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    html.Hr(),\n",
        "\n",
        "    dbc.Row([\n",
        "        dbc.Col([html.Label(\"Conta:\"), dcc.Dropdown(id=\"conta-dropdown\", value=\"Todos\")], md=3),\n",
        "        dbc.Col([html.Label(\"Tipo de Complexidade:\"), dcc.Dropdown(id=\"tipo-dropdown\", value=\"Todos\")], md=3),\n",
        "        dbc.Col([html.Label(\"Estado da Anomalia:\"), dcc.Dropdown(id=\"anomalia-dropdown\", value=\"Todas\")], md=3),\n",
        "        dbc.Col([\n",
        "            html.Label(\"Ação:\"),\n",
        "            html.Button(\"Baixar Relatório (CSV)\", id=\"btn-baixar-csv\", className=\"w-100 btn btn-success\")\n",
        "        ], md=3, className=\"d-flex flex-column justify-content-end\")\n",
        "    ], className=\"mb-4\"),\n",
        "\n",
        "    dcc.Graph(id=\"scatter-graph\"),\n",
        "    dash_table.DataTable(id=\"table\", page_size=10, style_table={\"overflowX\": \"auto\"},\n",
        "                         style_cell={\"textAlign\": \"left\"}, style_header={'backgroundColor': '#003366', 'color': 'white', 'fontWeight': 'bold'},\n",
        "                         sort_action=\"native\", filter_action=\"native\")\n",
        "], fluid=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. CALLBACKS - LÓGICA INTERATIVA DO DASHBOARD\n",
        "# ==============================================================================\n",
        "\n",
        "# Callback para processar o ficheiro carregado\n",
        "@app.callback(\n",
        "    Output('dados-processados-memoria', 'data'),\n",
        "    Output('output-upload-state', 'children'),\n",
        "    DashInput('upload-dados', 'contents'),\n",
        "    State('upload-dados', 'filename')\n",
        ")\n",
        "def processar_ficheiro_carregado(contents, filename):\n",
        "    if contents is None:\n",
        "        return None, \"Por favor, carregue um ficheiro (CSV ou Excel) para iniciar a análise.\"\n",
        "\n",
        "    content_type, content_string = contents.split(',')\n",
        "    decoded = base64.b64decode(content_string)\n",
        "\n",
        "    try:\n",
        "        if 'csv' in filename:\n",
        "            df_inicial = pd.read_csv(io.StringIO(decoded.decode('utf-8')))\n",
        "        elif 'xls' in filename or 'xlsx' in filename:\n",
        "            df_inicial = pd.read_excel(io.BytesIO(decoded))\n",
        "        else:\n",
        "            return None, html.Div(['Tipo de ficheiro não suportado.'], style={'color': 'red'})\n",
        "\n",
        "        # --- INÍCIO DO BLOCO DE LIMPEZA AUTOMÁTICA ---\n",
        "\n",
        "        # !! IMPORTANTE: AJUSTE ESTE DICIONÁRIO PARA OS NOMES DAS SUAS COLUNAS !!\n",
        "        colunas_para_renomear = {\n",
        "            'ID da Transação': 'id_transacao',\n",
        "            'Data': 'data_hora',\n",
        "            'Montante': 'valor',\n",
        "            'Categoria da Conta': 'conta',\n",
        "            'Entidade': 'id_entidade',\n",
        "            'Tipo': 'tipo_transacao'\n",
        "        }\n",
        "        df_inicial.rename(columns=colunas_para_renomear, inplace=True)\n",
        "\n",
        "        colunas_necessarias = ['id_transacao', 'data_hora', 'valor', 'conta', 'id_entidade']\n",
        "        colunas_em_falta = [col for col in colunas_necessarias if col not in df_inicial.columns]\n",
        "        if colunas_em_falta:\n",
        "            return None, html.Div([f'Erro: Faltam as seguintes colunas: {\", \".join(colunas_em_falta)}'], style={'color': 'red'})\n",
        "\n",
        "        df_inicial['data_hora'] = pd.to_datetime(df_inicial['data_hora'], errors='coerce')\n",
        "        df_inicial['valor'] = pd.to_numeric(df_inicial['valor'], errors='coerce')\n",
        "        df_inicial.dropna(subset=['valor', 'data_hora'], inplace=True)\n",
        "\n",
        "        # --- FIM DO BLOCO DE LIMPEZA ---\n",
        "\n",
        "        df_processado = executar_pipeline_de_analise(df_inicial)\n",
        "        json_data = df_processado.to_json(date_format='iso', orient='split')\n",
        "\n",
        "        return json_data, html.Div([f'Ficheiro \"{filename}\" carregado e processado com sucesso!'], style={'color': 'green'})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None, html.Div(['Ocorreu um erro ao processar o ficheiro. Verifique o formato e as colunas.'], style={'color': 'red'})\n",
        "\n",
        "# Callback principal que atualiza o dashboard\n",
        "@app.callback(\n",
        "    Output(\"scatter-graph\", \"figure\"),\n",
        "    Output(\"table\", \"data\"),\n",
        "    Output(\"table\", \"columns\"),\n",
        "    Output(\"conta-dropdown\", \"options\"),\n",
        "    Output(\"tipo-dropdown\", \"options\"),\n",
        "    Output(\"anomalia-dropdown\", \"options\"),\n",
        "    DashInput('dados-processados-memoria', 'data'),\n",
        "    DashInput(\"conta-dropdown\", \"value\"),\n",
        "    DashInput(\"tipo-dropdown\", \"value\"),\n",
        "    DashInput(\"anomalia-dropdown\", \"value\")\n",
        ")\n",
        "def actualizar_dashboard(json_data, conta, tipo, anomalia):\n",
        "    if json_data is None:\n",
        "        fig_vazia = {\"layout\": {\"annotations\": [{\"text\": \"Carregue um ficheiro para exibir os dados.\", \"xref\": \"paper\", \"yref\": \"paper\", \"showarrow\": False, \"font\": {\"size\": 20}}]}}\n",
        "        colunas_vazias = [{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]]\n",
        "        opcoes_vazias = [{\"label\": \"Todos\", \"value\": \"Todos\"}]\n",
        "        opcoes_anomalia_fixas = [{\"label\": \"Todas\", \"value\": \"Todas\"},{\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"},{\"label\": \"Apenas Normais\", \"value\": \"Normais\"}]\n",
        "        return fig_vazia, [], colunas_vazias, opcoes_vazias, opcoes_vazias, opcoes_anomalia_fixas\n",
        "\n",
        "    df_final = pd.read_json(json_data, orient='split')\n",
        "    df_final['data_hora'] = pd.to_datetime(df_final['data_hora'])\n",
        "\n",
        "    opcoes_conta = [{\"label\": c, \"value\": c} for c in [\"Todos\"] + sorted(df_final[\"conta\"].unique())]\n",
        "    opcoes_tipo = [{\"label\": c, \"value\": c} for c in [\"Todos\"] + df_final[\"complexidade\"].unique().tolist()]\n",
        "    opcoes_anomalia = [{\"label\": \"Todas\", \"value\": \"Todas\"},{\"label\": \"Apenas Anômalas\", \"value\": \"Anómalas\"},{\"label\": \"Apenas Normais\", \"value\": \"Normais\"}]\n",
        "\n",
        "    df_filtrado = df_final.copy()\n",
        "    if conta and conta != \"Todos\": df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo and tipo != \"Todos\": df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "    if anomalia and anomalia == \"Anómalas\": df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia and anomalia == \"Normais\": df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        df_filtrado, x=\"valor\", y=\"dia_do_ano\", color=\"anomalia_detectada\",\n",
        "        hover_data=['conta', 'complexidade', 'modelo_deteccao', 'data_hora'],\n",
        "        color_discrete_map={'Anómala': '#FF4136', 'Normal': '#0074D9'},\n",
        "        title=\"Visualização de Anomalias (Valor vs. Dia do Ano)\",\n",
        "        labels={\"valor\": \"Valor da Transação (MZN)\", \"dia_do_ano\": \"Dia do Ano\"}\n",
        "    )\n",
        "    fig.update_layout(transition_duration=500)\n",
        "\n",
        "    colunas_tabela = [{\"name\": col, \"id\": col} for col in [\"conta\", \"valor\", \"data_hora\", \"complexidade\", \"anomalia_detectada\", \"modelo_deteccao\"]]\n",
        "    data_tabela = df_filtrado[[c['id'] for c in colunas_tabela]].to_dict(\"records\")\n",
        "\n",
        "    return fig, data_tabela, colunas_tabela, opcoes_conta, opcoes_tipo, opcoes_anomalia\n",
        "\n",
        "# Callback para gerar e baixar o relatório CSV\n",
        "@app.callback(\n",
        "    Output(\"download-relatorio-csv\", \"data\"),\n",
        "    DashInput(\"btn-baixar-csv\", \"n_clicks\"),\n",
        "    State('dados-processados-memoria', 'data'),\n",
        "    State(\"conta-dropdown\", \"value\"),\n",
        "    State(\"tipo-dropdown\", \"value\"),\n",
        "    State(\"anomalia-dropdown\", \"value\"),\n",
        "    prevent_initial_call=True\n",
        ")\n",
        "def baixar_relatorio(n_clicks, json_data, conta, tipo, anomalia):\n",
        "    if json_data is None:\n",
        "        return None\n",
        "\n",
        "    df_final = pd.read_json(json_data, orient='split')\n",
        "\n",
        "    df_filtrado = df_final.copy()\n",
        "    if conta and conta != \"Todos\": df_filtrado = df_filtrado[df_filtrado[\"conta\"] == conta]\n",
        "    if tipo and tipo != \"Todos\": df_filtrado = df_filtrado[df_filtrado[\"complexidade\"] == tipo]\n",
        "    if anomalia and anomalia == \"Anómalas\": df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Anómala\"]\n",
        "    elif anomalia and anomalia == \"Normais\": df_filtrado = df_filtrado[df_filtrado[\"anomalia_detectada\"] == \"Normal\"]\n",
        "\n",
        "    return dcc.send_data_frame(df_filtrado.to_csv, \"relatorio_auditoria.csv\", index=False)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXECUÇÃO DA APLICAÇÃO NO COLAB\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    app.run_server(mode='inline')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "-SX43mKQqzhM",
        "outputId": "54b849fc-9b39-41b8-a7d4-bd0f5bfdf423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso.\n",
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:dash.dash:Dash is running on http://127.0.0.1:8050/\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **⏫: ⏫  ALGORITMO FUNCIONAL V2**"
      ],
      "metadata": {
        "id": "XUzz3mZlsDNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-AZDJE-QsPVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}